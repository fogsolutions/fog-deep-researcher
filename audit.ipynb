{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f30f220276469482f9bbf045c8f2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Specify the path to your local model or the model name from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Initialize the Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z9/rcc6fb195dnclys6htg1vv_w0000gn/T/ipykernel_67133/477777290.py:16: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-unstructured package and should be used instead. To use it run `pip install -U :class:`~langchain-unstructured` and import as `from :class:`~langchain_unstructured import UnstructuredLoader``.\n",
      "  loader = UnstructuredFileLoader(document_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./my-docs/Amynta Group - Solution Design.docx\n",
      "Loaded ./my-docs/Broker In A Box/Broker in a Box - Exec Overview (01-21-2025).pptx\n",
      "Loaded ./my-docs/Broker In A Box/Assured Partners - Broker in a Box - Endorsements - Proposed Solution Overview - 01292025.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/Broker In A Box Change Request#2 1.14.25 - SOW - AP Format.docx\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/Fog Solutions - Assured Partners - Broker in a Box - Proposal.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/SOW_3060310_Fog Solutions Inc._CAS-1477383-D1Q5K3_267970094 - signed.pdf\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/AssuredPartners - Fog Solutions - Broker in a Box - Exhibit A.docx\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/ECIF PO 101108580 AssuredPartners.pdf\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/AssuredPartners - Fog Solutions - Broker in a Box - AP Formated SOW - Fully Executed.pdf\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/Broker In A Box Change Request SOW - AP Format (002) - Fully executed.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinbooth/fog/gpt-researcher/.venv/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/kevinbooth/fog/gpt-researcher/.venv/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/kevinbooth/fog/gpt-researcher/.venv/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/kevinbooth/fog/gpt-researcher/.venv/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/OLD Fog Solutions -Assured Partners - Broker in a Box - Estimation Worksheet.xlsx\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/Broker In A Box Change Request #2 1.23.25 - SOW - AP Format.docx.pdf\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/AssuredPartners - Fog Solutions - Broker in a Box - SOW.docx\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/Archive/OUTDATED FORMAT Broker in a Box - AssuredPartners - Fog Solutions - CR2.docx\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/Archive/OUTDATED FORMAT Broker in a Box - AssuredPartners - Fog Solutions - CR1 with Admin.docx\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/Archive/OUTDATED FORMAT Broker in a Box - AssuredPartners - Fog Solutions - CR1.docx\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/Archive/[Not in effect, see AP formatted CR] Broker in a Box - AssuredPartners - Fog Solutions - CR1.pdf\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/ECIF POE/POE_End_Customer_3060310_Fog Solutions Inc._CAS-1477383-D1Q5K3_573072723 - signed.pdf\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/ECIF POE/POE_End_Customer_3060310_Fog Solutions Inc._CAS-1477383-D1Q5K3_-309549041 - signed.pdf\n",
      "Loaded ./my-docs/Broker In A Box/Proposal & Contracts/ECIF POE/POE_End_Customer_3060310_Fog Solutions Inc._CAS-1477383-D1Q5K3_962821672 - signed.pdf\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/AP Standing Team - Burnout Chart and Data.xlsx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/ESTIMATION_PROJ TRACKER - AP - BiaB Standing Team CR2.xlsx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Technology Overview and Infrastructure Summary.pdf\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Technology Overview and Infrastructure Summary.docx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Broker In a Box - Comprehensive Backlog of Work Items and Features - 10152024.xlsx\n",
      "Error loading ./my-docs/Broker In A Box/Delivery/Getting Started/Broker in a Box - Getting Started Guide - 11262924.pptx: Package not found at './my-docs/Broker In A Box/Delivery/Getting Started/Broker in a Box - Getting Started Guide - 11262924.pptx'\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Getting Started/Broker in a Box - Exec Overview (01-09-2025).pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Support Materials/Broker in a Box Charter.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Support Materials/Broker in a Box Transcript - Call with Rob - July 29.docx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Designs/BAIB - Device Collage.png\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Designs/BAIB - AI Policy Suggestions.png\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Designs/BAIB - Login.png\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Designs/BAIB - Create a Project Dialog.png\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Project Kickoff/Fog Solutions - Assured Partners - Broker in a Box - Kickoff.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/AP - Production Release Required Materials/SOP-- template to be replicated.docx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/AP - Production Release Required Materials/RPA_LIFTOFF_PaperCheckDeposit_DBRecon_SDD_V1.0.docx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/AP - Production Release Required Materials/Run book - Broker in a Box - FOG DRAFT - 120924.docx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/AP - Production Release Required Materials/Run Book-- template to be replicated.docx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/AP - Production Release Required Materials/SOP - Broker in a Box - FOG DRAFT - 120924.docx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/AP - Production Release Required Materials/Release (Paper Check Deposit Bot) - Deployment Plan.xlsx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 3 Mid-Sprint- 1003241.pptx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinbooth/fog/gpt-researcher/.venv/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 10 Mid-Sprint - 022125.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 6b Sprint-End  -122024.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 4 Sprint-End & Monthly Stakeholder Review -102524.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 2 End-Sprint & Monthly Stakeholder Review - 0927241.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box - Mid Project AP Exec Demo - 102224.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 1 Sprint-End Review - 0912241.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 8 Mid-Sprint-012425.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 6b Mid-Sprint  -121324.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 3 End-Sprint- 101124.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 7 mid-Sprint -011025.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 5 Mid-Sprint -110124.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 8 Sprint-End - 013125.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 6a Sprint-End  -120624.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 2 Mid-Sprint Review - 0920241.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 6 Mid-Sprint -111524.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 4 Mid-Sprint- 1018241.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 5 End-Sprint -110824.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 9 Mid-Sprint - 020725.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 7 Sprint-End -011725.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 9 Sprint-End - 021425.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box Sprint 6 Sprint-End & Monthly Stakeholder Review -112224.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Vision Deck/Fog AP - Vision Deck - Broker in a Box - 1024.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Vision Deck/AP-AI Roadmap & Broker in a Box review - DRAFT - 010925.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/Vision Deck/Fog AP - Vision Deck - Broker in a Box.pptx\n",
      "Loaded ./my-docs/Broker In A Box/Delivery/QA & UAT/UAT Suggestion & Rules Issues List - 120424.xlsx\n",
      "Loaded ./my-docs/Broker In A Box/ARB Review/ARB Meeting Scheduling and Required Information.docx\n",
      "Loaded ./my-docs/Broker In A Box/ARB Review/PolicyPAL Requirements.pptx\n",
      "Loaded ./my-docs/Broker In A Box/ARB Review/Assured Partners current ARB Questionnaire_PolicyPal.xlsx\n",
      "Loaded ./my-docs/Broker In A Box/ARB Review/Assured Partners ARB Questionnaire.xlsx\n",
      "Loaded ./my-docs/Broker In A Box/ARB Review/ARB Template.pptx\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "def load_document_paths(directory):\n",
    "    document_paths = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.pptx', '.xlsx', '.docx', '.pdf', '.png', '.jpg', '.jpeg', '.txt', '.csv', '.json', 'yaml', '.html')):\n",
    "                document_paths.append(os.path.join(root, file))\n",
    "    return document_paths\n",
    "\n",
    "document_paths = load_document_paths('./my-docs')\n",
    "all_documents = []\n",
    "for document_path in document_paths:\n",
    "    loader = UnstructuredFileLoader(document_path)\n",
    "    try:\n",
    "        document = loader.load()\n",
    "        print(f\"Loaded {document_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {document_path}: {e}\")\n",
    "        continue\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "    documents = text_splitter.split_documents(document)\n",
    "    all_documents.extend(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load/save documents to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01min_memory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InMemoryDocstore\n\u001b[0;32m----> 7\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241m.\u001b[39membed_documents([doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m all_documents])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the FAISS index\u001b[39;00m\n\u001b[1;32m     10\u001b[0m dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(document_embeddings[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "document_embeddings = embeddings.embed_documents([doc.page_content for doc in all_documents])\n",
    "\n",
    "# Initialize the FAISS index\n",
    "dimension = len(document_embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "doc_store = InMemoryDocstore()\n",
    "# Create the FAISS vector store\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings, \n",
    "    index=index, \n",
    "    docstore=doc_store, \n",
    "    index_to_docstore_id={})\n",
    "\n",
    "\n",
    "\n",
    "# # Add documents and their embeddings to the vector store\n",
    "# vector_store.add_documents(all_documents)\n",
    "# # Save the FAISS index and documents\n",
    "# vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "chroma = Chroma(embedding_function=embeddings, persist_directory=\".chroma\")\n",
    "amynta_report = \"\"\"\n",
    "\n",
    "Solution Design\n",
    "\n",
    " \n",
    "1. Project Summary\n",
    "The Amynta Group, an industry-leading team of warranty and specialty risk companies, as well as managing general agents, has experienced significant growth since its founding in 2018. Through strategic acquisitions, Amynta has expanded its warranty and services offerings across the automotive, consumer products, and specialty equipment industries. To support this growth and enhance business visibility into data such as policies, actuarial, and claims, Amynta requires an enterprise data strategy that provides a unified and scalable data platform. This will enhance data accessibility, governance, and analytics across the organization. Amynta has been exploring Microsoft Fabric as the foundation for this data strategy.  Ultimately, Amynta aims to make reporting and analytics accessible across the organization. By implementing a Microsoft Fabric-based solution, Amynta will be able to achieve this goal while centralizing data management, ensuring a single source of truth, and reducing data silos. \n",
    "\n",
    "The central goal of this initiative is to furnish the Microsoft Fabric platform, which is intended to support a consolidated enterprise-wide Lakehouse environment.  The Lakehouse will serve as the trusted foundation for analytic capabilities like BI, ML, and AI.  It includes core components for ingesting, transforming, and managing data for the purpose of supporting downstream analytics.  \n",
    "\n",
    "Scope of the Solution\n",
    "1.\tFabric environment is provisioned with public endpoints, and OneLake is leveraged as the foundation of the solution and for future business use case scalability and diversity \n",
    "2.\tData will be ingested and processed daily for operational purposes and processed at month end which is the required SLA. \n",
    "3.\tSolution will leverage a single Direct-Lake semantic model and Power BI reports will be as up to date as their underlying delta tables.\n",
    "4.\tStar schema that combines device data and inventory data will be created to ensure the success of streamlined report development\n",
    "\n",
    "Stakeholders and Audiences\n",
    "The primary stakeholders for this solution are:\n",
    "•\tChief Financial Officer – Monthly and Quarterly Business Reviews\n",
    "•\t36 Business Unit Leaders – used for Monthly and Quarterly Business Reviews and also use it daily for operational needs.   NOTE: Initial phase will only focus on the Amynta Dealer Services BU.\n",
    "•\tDirect reports to Business Unit Leaders as needed – used for operational needs\n",
    "•\tActuarial Team – used to support risk management across the businesses\n",
    "2. Solution Design Overview\n",
    "2.1 Solution Objective\n",
    "Currently, Amynta has a diverse set of technologies to meet the varying needs of its business analytics across different business units. Some teams prefer using Excel for its ease of use, flexibility, and data manipulation features, enabling quick insights from smaller datasets. In contrast, other business units integrate Power BI with Microsoft Synapse, allowing for advanced data modeling, large-scale data processing, and real-time analytics to support large and diverse datasets. This combination supports more complex, enterprise-wide reporting and predictive analysis. Additionally, other business units are leveraging SQL Server to manage their datasets and Power BI to create dynamic dashboards and interactive reports. In summary, there’s a mix of analytic support models that are serving the business analytics needs.  At the same time, there is repetitive work being done, data is highly siloed, and business users are spending time building and managing these environments.  With all of this in mind, Amynta wants to build a consolidated Lakehouse environment to provide value to the organization\n",
    "\n",
    "Technical success metrics defined by Amynta Group:\n",
    "•\tPrecision and Reliability in data collection \n",
    "•\tIntegration of multiple sources into a unified platform on a regular basis\n",
    "•\tReduction of manual extraction and processing of data\n",
    "•\tPerformance: Reports should be rendered in under 30 seconds for an end user\n",
    "•\tPerformance: Pipeline refresh should occur in under 1 hour\n",
    "•\tOperational awareness of data platform activities\n",
    "•\tScheduled refresh and delivery of executive reports\n",
    "\n",
    "Business success metrics defined by Amynta Group:\n",
    "•\tEfficiency gains in policy renewal\n",
    "•\tInsights achieved in product management\n",
    "•\tDetecting low performing agents \n",
    "•\tReducing time from submission to quotes to binds\n",
    "\n",
    "2.2 Current State Solution Diagram\n",
    "The following diagram depicts the current state of the Analytics team that exists within the Actuarial business.  It follows a legacy enterprise data warehouse pattern.  Currently, only two sources (Guardsman and WarranTech) are being brought into this environment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Data for Guardsman is being replicated into the Synapse environment today using Qlik Replicate.  The team is looking to retire the Qlik solution.\n",
    "\n",
    "Data for WarranTech is pulled into the Synapse environment three times per day.  It is being ingested using Azure Data Factory.\n",
    "\n",
    "2.3 Future State Solution Architecture\n",
    "This is the diagram of the Fabric solution with the 3 sources:\n",
    " \n",
    "\n",
    "1.\tThe pipeline will run and execute a Notebook, this Notebook is responsible for ingesting data from ALIS into the Bronze Lakehouse artifact as a queryable Delta Table. \n",
    "2.\tThe pipeline will execute a Copy Activity that leverages an On-Premises gateway to extract data from the IMS or Amynta Dealer Services data from the SQL Server source.\n",
    "3.\tThe Fabric Data Factory Pipeline will be responsible from orchestration from this point forward, it will be stored in the Fabric Workspace, but the diagram is to depict its responsibilities and show that secrets and config values will be stored in Key Vault.\n",
    "4.\tThe pipeline will execute a transformation step using Dataflow Gen2/Pipeline/Stored Procedure/Notebook to transform the data to support the reporting layer and semantic model.\n",
    "5.\tReports are generated off a separate Direct-Lake semantic model. The initial report to support phase 1 is the Product Dashboard report, which will be built off a semantic model.\n",
    "6.\tThe following Azure Resources will be utilized for the project:\n",
    "a.\tAzure Key Vault - used for storing configurations and secrets for the pipeline and notebooks.  API credentials to Logic Monitor are an example of this type of configuration\n",
    "b.\tFabric Capacity - a dedicated capacity will be provisioned and attached to the workspaces we will use during the project, the ONLY Fabric specific abilities in this Azure Resource are scaling SKU up and down, and starting and stopping the capacity\n",
    "c.\tPower Automate Flow - used to start and stop the Fabric Capacity Resource so that Amynta can recognize further cost savings when using CoPilot SKUs\n",
    "\t\t\n",
    "\n",
    "3. User Roles and Security Considerations\n",
    "Microsoft Fabric supports a range of security measures to control what features of the workspace users have access to, and who can read or modify what data. \n",
    "In general, the following are recommended: \n",
    "•\tLeverage the medallion architecture to curate and combine common data across varying sources, securing access to roles that need access to each medallion layer, via separate Lakehouses\n",
    "•\tUse of Azure Key Vault for securing credentials that Pipelines and Notebooks via Entra Service Principal/Workspace Identity\n",
    "•\tUse of Workspace Access Control to control who can view or run which notebooks and view data in the Gold layer \n",
    "•\tLeverage Direct-Lake for handling varying source refresh times using Row-Level Security (RLS) for varying departmental data security \n",
    "\n",
    "3.1 Definition of User Roles\n",
    "Specific roles will be leveraged by configuring Entra Groups to each role. The following roles and groups will be leveraged for the implementation:\n",
    "Resource Group Contributor Group \n",
    "•\tCan provision Fabric Capacity as an Azure Resource \n",
    "Custom Fabric Capacity Admin Group \n",
    "•\tCan create & configure workspaces to use a specific capacity \n",
    "•\tCan scale capacity performance SKUs/tiers \n",
    "•\tMust use PIM to enable this for scaling SKU up or down and for workspace assignment\n",
    "•\tCustom definition:\n",
    "•\t\"Microsoft.Fabric/capacities/read\"\n",
    "•\t\"Microsoft.Fabric/capacities/write\"\n",
    "•\t\"Microsoft.Fabric/capacities/pause/action\"\n",
    "•\t\"Microsoft.Fabric/capacities/resume/action\"\n",
    "•\t\"Microsoft.Fabric/workspaces/read\"\n",
    "•\t\"Microsoft.Fabric/workspaces/write\"\n",
    "•\t\"Microsoft.Fabric/workspaces/assign/action\"\n",
    "•\t\"Microsoft.Fabric/operations/read\"\n",
    "Workspace Admin Group \n",
    "•\tResource management, create, modify, and delete resources \n",
    "•\tUser and Access Control \n",
    "Workspace Contributor Group \n",
    "•\tMost developers have this role \n",
    "•\tResource management, create, modify, and delete resources \n",
    "Workspace Viewer Group \n",
    "•\tConsume and view all content inside of the workspace but cannot, create, modify, or delete artifacts \n",
    "•\tReporting access to view reports\n",
    "•\tRole-Based Access Control (RBAC) Strategy\n",
    "4. Data Sources and Ingestion\n",
    "4.1 Identification of Data Sources\n",
    "For this phase of the project, we have strategically selected two data sources that offer the most significant value for our investment. These choices are based on their potential to maximize efficiency and impact, ensuring optimal resource utilization and return. These sources are:\n",
    "o\tALIS Policy Administration:\n",
    "o\tOverview - ALIS is a comprehensive, cloud-based software solution designed primarily for Managing General Agents (MGAs), Managing General Underwriters (MGUs), wholesalers, and program administrators in the insurance industry.  ALIS is going to be the standard for policy administration across Amynta.  Currently, only a handful of business units use the platform.  \n",
    "o\tData Access – ALIS is a SaaS solution and provides several APIs to access the policy data.  The ALIS API uses token-based authentication, with each token linked to a user account, enabling granular permission management for integrations.\n",
    "o\tDatatype – JSON\n",
    "o\tData Ingest Frequency – Daily\n",
    "o\tExpected Volumes – \n",
    "o\tHistorical:  < 500 GB total with historical data\n",
    "o\tIncremental:  < 2-5 GB total with historical data.  Will accelerate as more business units join the platform.  Roadmap is being determined.\n",
    "\n",
    "o\tIMS Insurance Management System:\n",
    "o\tOverview - Designed specifically for MGAs, program administrators and carriers, Insurance Management System (IMS) is a full-featured, scalable policy administration system. IMS includes everything you need for end-to-end policy lifecycle management, from rating, insured tracking and clearance and company and market management.  This is a legacy system currently in use with a few business units and will ultimately be replaced by ALIS.\n",
    "o\tDatatype – Structured data from a SQL Server source\n",
    "o\tData Ingest Frequency – Daily\n",
    "o\tExpected Volumes –\n",
    "o\tHistorical: < 50 GB\n",
    "o\tIncremental: <200 MB\n",
    "\n",
    "o\tFuture Data Sources\n",
    "The following table shows the list of data sources in priority order that will be leveraged throughout the various phases of the engagement.\n",
    "Source Systems\tAverage Amount of Reporting from Source\n",
    "WarranTech\t40%\n",
    "Guardsman\t35%\n",
    "TPA\t35%\n",
    "\t\n",
    "All sources are internet accessible API extracts with an Amynta Group token, CSVs, or SQL-based sources. The token is managed by Azure Key Vault for secure access.\n",
    "4.2 Data Ingestion Strategy\n",
    "Data ingestion is performed through Fabric Data Factory for orchestration of copy data pipelines and notebooks. Each source has its own orchestration pipeline so it can run as a stand-alone process but there is also a master orchestration pipeline that runs one time per day that executes each child pipeline for each source. For the sources in this current phase:\n",
    "•\tALIS – this API call to ingest the raw data is implemented in a Spark Notebook inside of Fabric. Since the API call is a more complex source with specific vendor requirements on how to pull the data, a Notebook is used.\n",
    "•\tIMS – this data is extracted over a Fabric Gateway and using Fabric Data Factory and copy activity pipelines to securely access the SQL Server data hosted on a private virtual network and Azure VM.  \n",
    "Within each pipeline and notebook in the ingestion layer, a metadata framework is leveraged that uses pipeline variables (json) and Lakehouse control tables, in the Utility Lakehouse. There are 2 key phases of data ingestion in the medallion architecture, these are: \n",
    "1.\tSource System to Raw: This part of the solution is responsible for extracting the raw data from the source systems into the Files location of the Bronze Fabric Lakehouse. These extracts are done in full and incrementally depending on which source, but these Files are stored in the Files section with a timestamp path and the source system. Example: \n",
    "IMS /2024/11/27 – for this project, the files on the same day are overwritten \n",
    "2.\tRaw to Bronze: In the Bronze layer the data is simply ingested to the ʻBronze Delta tablesʼ “as-is” along with audit columns like ingestion date, batch ID, etc. Incoming data is composed of parquet files for approximately 500 tables daily. The primary purpose of bronze tables is the ability to provide a historical archive of source data lineage, auditability, and reprocessing if needed without rereading the data from the source system.  \n",
    "4.3 Data Ingestion Metadata Framework\n",
    "Amynta has numerous data sources containing various types of data stored in various ways. All of these data share a set of common attributes: data location, data format, can I identify new or changed data, if yes to last how do I identify the changed data, what are the keys to update for changed data. \n",
    " \n",
    "Ingesting this data into a common data repository is also a standard process: connect to the source, copy the source, write the source to bronze, merge the source to Silver. \n",
    "While this is an oversimplification, it is meant to demonstrate the opportunity to control the data ingestion process through a metadata framework. \n",
    "\n",
    "4.3.1\tIngestion Metadata \n",
    "The ingestion metadata is driven by 3 primary tables:  \n",
    "•\tSource – connection info for the source  \n",
    "•\tSource_Object – details required for processing object (PKs, Incremental attributes)  \n",
    "•\tObject_Field – data type, column order, nullable or not  \n",
    "\n",
    "4.3.1.1 Source Table  \n",
    "   \n",
    "This table will have one row for each specific source instance.  \n",
    "  \n",
    "The bulk of the information is carried within the source_connection_json column.  The reason to store it in JSON is to prevent this table from constantly having columns added to support new source platforms. For example, for ADLS as a source, the table would need to have added Storage Account, Storage Container, StoragePath; but a SQL Server source would need Servername, DBName, UserId, Password (keyvault secret name).  \n",
    "\n",
    "Continually adding new source platforms leads to more columns or often misuse of existing columns out of convenience (i.e. a source type doesn’t need DBName for this SourceType so developers may store a different value here out of convenience that the new source type does need).  \n",
    " \n",
    "Here is an example of source_connection_json:  \n",
    "{    \"development\": {   \n",
    "                    \"server_name\": \"fogdemoserver.database.windows.net\",   \n",
    "                    \"user_id\":\"FabricLoader\",   \n",
    "                    \"password\":\"sc-importer-password\",   \n",
    "                    \"db_name\":\"abc_trans_db\"   \n",
    "                       },   \n",
    "    \"production\": {   \n",
    "                    \"server_name\": \"fogdemoserver.database.windows.net\",   \n",
    "                    \"userId\":\"FabricLoader\",   \n",
    "                    \"password\": \"sc-importer-password\",   \n",
    "                    \"db_name\":\"abc_trans_db\" }  \n",
    "                    }  \n",
    "  \n",
    "Within source_connection_json there must always be a minimum of one value kept in a KeyVault Secret (or secure equivalent).  \n",
    "  \n",
    "Depending on the source type there will be different name/value pairs.  The JSON also allows storage of connections for one to many environments.  Every environment has its own metadata database. Within every MetaData database there is a function called fn_Environment() which is hard coded to return “Development”, “Production” or other required environments.  \n",
    " \n",
    "This function will be restricted to database admins. The Fabric Managed Identity will have rights explicitly denied to ‘alter the function’.  This is to prevent accidental changes during deployment or mistakes during development.  \n",
    " \n",
    " When a Pipeline or a Notebook needs Source Connection Information, they will call a stored procedure passing in the Source Name and will get the connection information or for pipelines it will return Connection Info and the Objects to be processed.  \n",
    "\n",
    "4.3.1.2 Source Object Table\n",
    "The source_object table identifies the specific objects/tables to be loaded for a source.  \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "The source_object table identifies the specific objects/tables to be loaded for a source.  \n",
    " \n",
    "If source type is a database there will be an object_name and schema_name. If the source is an ADLS storage account with a Path in connection properties, then object_name could be a folder name and object_schema would be blank.  \n",
    "  \n",
    "The load_grouping column can be used to differentiate into batches.  For a source, maybe five tables can start loading at 9:00 pm but the other eighty tables we need to wait for 3:00 am.  A trigger could execute a pipeline to do source and load_grouping 1, then run a separate pipeline later with load_grouping of 2.  \n",
    " \n",
    "The next three columns relate to incremental loads.  Wherever possible is_incremental should be set to “1”, as a “0” would mean full load which is basically a truncate target and reload (impractical for large tables).  \n",
    " \n",
    "The watermark_column drives the incremental load, it is added to criteria for the query that is generated.  Some metadata tables store the max value of the watermark_column, a better practice is to query the target for the max date when incremental ingestion is starting; this handles situations where the last load had to be deleted out of target because of some issue.  \n",
    " \n",
    "Whenever an incremental load is run, the new data is brought to Bronze (where day after day there may be new versions of the record).  Silver only has the most current version of the record.  To process an Upsert into Silver the merge process must have columns that identify new records or updated records when compared to current Silver.  \n",
    " \n",
    "The second half of the table shown in the 2nd diagram starts with three custom columns. The ingest process will automatically generate a SELECT * FROM source WHERE watermark_column > ‘2024/12/3 03:42:472’.  If custom logic is required, adding that SQL to custom_query will override the auto created statement.  \n",
    "Generally, ingestion brings down all columns with no transformation. However, a very wide table might require the process to bring 10 columns not 500. Or eliminating blobs that are stored on the object.   \n",
    "  \n",
    "The copy from source uses pipelines based on the source type.  Using metadata so that if we have ten SQL Servers as sources, they can all use a common pipeline.  The custom_pipeline column allows the generic pipeline to be overridden and have a different pipeline called. Possibly a very complex REST API might need their own custom pipeline.  \n",
    "\n",
    "Processing data from Bronze to Silver can also be done by a generic notebook; given that once they have landed in Bronze all data is in same format (Lakehouse delta tables).  If custom processing is required a specific notebook can be named.  \n",
    "  \n",
    "The is_active if “0” means object will not be processed. \n",
    "\n",
    "4.3.1.3 Object Fields Table\n",
    "The object_fields table is associated with to a source_object. The source_id is carried for convenience but is not required since you could join through source_object.  \n",
    " \n",
    "\n",
    "This table contains basic column information which allows the process Bronze to Silver to create the Silver target table if it is not there. If the table is present, it allows a check to see if there have been schema changes and add new columns to existing table. \n",
    "\n",
    "4.3.2 Data Quality Metadata\n",
    "For the initial project some very simple data quality is being implemented.  In future phases, these tables may be extended to support other data quality checks or possible future integration Purview and MDM integration may negate the need for these tables at all.  \n",
    " \n",
    "Data quality can be applied between Bronze and Silver or while loading Gold. \n",
    " \n",
    "There are two tables. The dq_object_field_table which lists a specific source object column and a data quality validation check or cleanse to apply.  The second table is the dq_field_lookup table which carries reference information to support application of rules and cleanse. \n",
    "\n",
    "dq_object_field columns \n",
    "dq_object_field_id \tprimary key – surrogate key auto generated \n",
    "object_field_id \t foreign key identifies column this record applies to \n",
    "fail_on_null \t if the value for the column is NULL then fail \n",
    "perform_lookup \t do a lookup and if current value of column is found, replace with the found lookup value \n",
    "pattern \t validate that the value of the column matches the regex pattern and fail if it does not \n",
    "string_clean \t trim any white space and remove all special characters \n",
    "left_string \t return only the left # of characters for the int value in this column \n",
    "right_string \t return only the right # of characters for the int value in this column \n",
    "validate_lookup \t check if the value in this column in in the domain range of lookup values \n",
    "created_timestamp \t when was record created \n",
    "last_updated_timestamp \t when was record changed \n",
    " \n",
    "Currently, any failures will be recorded in the table dq-log which records which exact records from bronze, and which load failed and which rule it failed on.  When a rule passes, the log is checked to see if the record (based on primary keys) previously failed and if so that error is marked as now being fixed. \n",
    " \n",
    "At this time, string trims or lookup replacements are not being tracked but may be added in future iterations. \n",
    " \n",
    " The reference table which supports the perform_lookup and the validate_lookup is: \n",
    "dq_field_lookup \n",
    "dq_field_lookup_id \tprimary key – surrogate key auto generated \n",
    "object_field_id \tforeign key identifies column this record applies to \n",
    "original_value \tFor perform_lookup this is the value in the record that will be replaced. \n",
    " \n",
    "For validate_lookup this is a value that is valid. \n",
    "replacement_value \tFor perform_lookup if the source record column has the original value then this is the value that will replace it. \n",
    "created_timestamp \t when was record created \n",
    "last_updated_timestamp \t when was record changed \n",
    " \n",
    "In the selection from the table below. Records 1 and 2 are both lookups. Record 1 of source_object_id column has a value of “Toronto” in Bronze it will be “metro Toronto” in Silver.  Record 2 will change a column from Null in Bronze to “Ted” in Silver.  Records 3 through 6 would all be related to a single perform_lookup rule on source_object_id 98 saying these are the only 4 values allowed. \n",
    "    \n",
    "Example of the dq_log table: \n",
    "  \n",
    " \n",
    "4.3.3 Ingestion Components\n",
    " \n",
    "\n",
    "The overall ingestion process is made up of the following Fabric components: \n",
    "•\tFabric Azure SQL DB:\n",
    "This is an Azure SQL database hosted within Fabric.  As a SaaS platform Fabric does not require a provisioned Azure SQL server to host it. \n",
    "This database is used to host the metadata which has the details about what data to load, where it is, how to connect, etc. \n",
    "•\tFabric Pipelines:\n",
    "Pipelines are the continued evolution of ADF Pipelines, Synapse Pipelines, and now Fabric Pipelines.   The primary purpose of most of the pipelines shown is orchestration, querying the metadata to understand what needs to run, executing and controlling the parallelism of execution of processing, and logging and alerting. The one data activity handled by the pipelines themselves is the initial copy of data in to Bronze which is done by a pipeline COPY activity. \n",
    "•\tPy/Spark Notebooks: \n",
    "Fabric as a SaaS platform does not require provisioning of Spark clusters for execution.  The notebooks handle the processing from Bronze to Silver and Silver to Gold and are executed by the pipelines which pass parameters to specify what objects and actions to take. \n",
    "•\tFabric Lakehouse and OneLake: \n",
    "The medallion architecture depicted in the diagram will be within a Fabric component called a Fabric Lakehouse.  The lakehouse is a logical grouping of data that resides on Fabric OneLake which is the central repository for all data in the Fabric tenant. The OneLake is Azure ADLS Gen2, however, because Fabric is SaaS there is no provisioning of the storage accounts or containers. \n",
    "\n",
    "4.3.4 Ingestion Processing\n",
    "The high-level document below is well annotated to describe the general flow and processing of the ingestion framework.  Below is a quick summary. \n",
    "\n",
    " \n",
    "\n",
    "1.\tThis area is just a generic representation of various possible data sources.  It is meant to be a representation of the fact that because Fabric out of the box has over 135 native connecters the ingestion framework will be able to meet most of Amynta’s needs. For phase 1, we are processing ALIS and IMS data sources.\n",
    "2.\tAs mentioned in preceding section, the metadata drives the ‘what’ that gets processed. The important note here in reference to the diagram is that the green symbol is shown multiple times to represent that it is used throughout the process. However, it is a single repository. \n",
    "3.\tThis parent pipeline is primarily an orchestrator calling other processes.  It is scheduled to execute (the initial implementation is batch ingestion) with a specific source parameter being passed in as the data to be ingested. \n",
    "4.\tAll data ingestion starts with ingestion to bronze layer so the generic ingest to bronze is called.  The only purpose of this pipeline is to establish the type of platform the source to be loaded is on and call the appropriate pipeline to load that source. \n",
    "Pipleines have some limitations to the number and ways logic control structures can be used, for example a ForEach cannot contain a ForEach. Combining IF, SWITCH (similar to a CASE or IF ELSE), with ForEach also have limits. This is why we see pipelines calling pipelines in what might seem to be extra steps. \n",
    "5.\tThese pipelines do the actual ingestion of data into the Bronze layer. Bronze is hosted on a Fabric lakehouse named lh_bronze. A LOOKUP activity is done to get a list of objects to process and they are processed in parallel in a FOREACH loop.  A COPY activity is used to ingest the data. Metadata states whether the load is full or incremental and if incremental also has other attributes required to do an incremental load. \n",
    "Parallelism can be adjusted based on monitoring under load to find the right balance between cost and performance. \n",
    "6.\tOnce the source has been ingested to bronze, the parent pipeline then calls the process silver pipeline. The silver pipeline does a LOOKUP activity to the metadata, to get a list of all the objects and then calls the notebook number 7 in a parallel fashion through a FOREACH loop. \n",
    "7.\tThe bronze to silver notebook uses py/spark to read metadata about the specific object to be processed. Loads only the last batches changes (whether that be incremental or full), takes that dataset of records and then applies any data quality checks or cleanse rules in the metadata, once rules applied, it merges (incremental) or replaces (full) the silver version of the object. \n",
    "8.\tSilver to gold pipeline is similar to the Bronze to Silver except the Gold has one extra responsibility.  The parent pipeline was executing for one specific source; however, gold objects might be dependent on multiple sources.  This pipeline will check a metadata table for any gold objects that were dependent on the source just processed, if there are any, it will check if the other sources it is dependent on have finished processing.  If all dependencies are met, then it will execute the process gold notebook. If not, the pipeline will exit, and the gold object will run when the pipeline executing the final dependency runs.  \n",
    "9.\tThe process gold notebook looks up the object to process and necessary attributes including the sql logic for loading the gold object.  All gold objects are loaded from silver. Fabric supports joining across lakehouse schemas, so logic to load a table can have joins across multiple sources.  The metadata also supports executing multiple statements for even more complex scenarios.  \n",
    "10.\tThis is a future representation of how streaming data can be integrated into the platform. \n",
    "\n",
    "5. Data Storage and Management\n",
    "5.1 Storage Solutions \n",
    "  \n",
    "OneLake is the optimized storage layer that provides the foundation for storing data and tables in the various Fabric Lakehouse artifacts. OneLake is based on the Delta Table format that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. Delta Tables are fully compatible with Apache Spark APIs and developed for tight integration with all of the Fabric platform, allowing you to easily use a single copy of data for all operations and providing processing at scale. \n",
    "\n",
    "5.2 Data Organization and Structuring\n",
    "Data in the Bronze Lakehouse is organized 2 different ways:\n",
    "1.\tWithin Lakehouse Files, each source system contains a folder of the source name, with each schema then table listed within that source. Schema-less sources will be sourcename/filename.\n",
    "2.\tWithin Lakehouse Tables, all tables will belong in the root but each table name will be prepended with “sourcename_<tablename>” as Fabric Schemas are a private preview in Fabric\t\n",
    "The Silver Lakehouse is leveraged to combine and transform different sources into a single tables for entities like policies, claims, agents, etc. The silver layer only leverages tables from the bronze layer.\n",
    "\n",
    "The Gold Lakehouse is the primary serving layer of the solution. These tables will be keyed using surrogate keys generated from the gold layer processing of the master pipeline and will be the base storage for the star schema, where the semantic model connects to.  \n",
    "\n",
    "5.3 Common Data Model\n",
    "A Common Data Model (CDM) is a standardized framework that defines the data structure, relationships, and definitions for data across an organization. By unifying data across different departments or business units, a CDM ensures that information is consistent, accurate, and easily accessible, providing a solid foundation for robust corporate reporting. \n",
    "\n",
    "The CDM will be employed to serve as an integration point for the various applications in use across Amynta’s 36 business units.\n",
    "\n",
    "The advantages of leveraging a CDM for corporate reporting, especially in a continually expanding company, are substantial. As a CDM program matures it will become the key central repository for corporate level reporting.  Over time there will be numerous dashboards and business reports created using it as the data source, many of them providing analytics across multiple business units. \n",
    "\n",
    "The following diagram illustrates the entities and keys that have been built for the initial phase of the enterprise data strategy program.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "6. Data Encryption and Security\n",
    "6.1\tEncryption Methods \n",
    "Microsoft Fabric OneLake is encrypted at rest by default. Microsoft Fabric enforces enterprise-grade security measures, and encryption at rest is a fundamental aspect of its data protection strategy. The encryption is FIPS 140-2 compliant and Microsoft-managed keys are rotated appropriately. Data in transit between Microsoft services is always encrypted with at least TLS 1.2, with negotiation to TLS 1.3 when possible. Inbound OneLake communication enforces TLS 1.2 and negotiates TLS 1.3 when possible.\n",
    "\n",
    "Fabric Data Factory pipelines and Fabric Spark Notebooks are configured to utilize Azure Key Vault for secure access to encrypted data, including API tokens, usernames, and passwords. This integration ensures that sensitive information is managed and accessed securely, enhancing overall data protection within the environment.\n",
    "By Source:\n",
    "ALIS – API token \n",
    "IMS – SQL Server, server name, username, password\n",
    "6.2 Data Security Techniques\n",
    "During the pilot phase Direct-Lake with RLS was demonstrated with the IMS source to show how different types of servers can be secured with RLS roles. For the implementation phase, Amynta Group elected not to leverage RLS at this time to maximize data analysis and promote access to data.\n",
    "\n",
    "There is currently not any PII, HIPAA, or sensitive data used in this solution.\n",
    "\n",
    "7. Analytics and Reporting \n",
    "The following section defines the key technologies being used to support the Amynta Group solution for Production Reporting.\n",
    "\n",
    "7.1 Analytics Platform\n",
    "Microsoft Fabric will serve as the foundation for all analytic workloads at Amynta Group.  It’s a comprehensive solution that provides a unified platform for data management, ingestion, transformation, and reporting.\n",
    "•\tFabric Ecosystem:\n",
    "o\tOneLake – data management\n",
    "o\tData Factory – orchestration and extraction\n",
    "o\tNotebooks – API extraction and data engineering\n",
    "o\tLakehouse – data manipulation\n",
    "o\tGateway – on-premise access\n",
    "\n",
    "7.3 Fabric Domains\n",
    "Domains provide a mechanism to convey ownership and responsibility over several workspaces.  It’s especially useful when there is some decentralization for managing data assets within Fabric.  Domains will create logical groupings of workspaces and allow for workspaces to be managed at a higher level.  Two types of domains will be created.  \n",
    "•\tEnterprise Analytics Domain – this domain will be owned and managed by IT and the team developing the centralized data platform environment.  \n",
    "•\tBusiness Unit Specific Domain – this domain will be owned and managed by a particular business unit that has requirements to create and manage data artifacts that may be unique to their business.  \n",
    "7.4 Fabric Workspaces\n",
    "Workspaces are a fundamental Fabric concept and serve as containers within the Fabric ecosystem.  They provide a place to create and manage collections of Fabric workloads such as task flows, reports, Lakehouses, and warehouses. It’s important to set standard conventions early on as you start to roll out Microsoft Fabric.  \n",
    "•\tWorkspace Creation – Limit workspace creation to a select group of users to better manage the environment but also provide capabilities where needed to more advanced business units.  Fog recommends a combination of centralized and decentralized individuals have this permission.  A likely scenario would include:  \n",
    "o\tIT stakeholders  \n",
    "o\tBusiness unit power users  \n",
    "o\tAnalytics Center of Excellence members  \n",
    "\n",
    "The following diagram illustrates the workflow design that has been implemented for phase 1.  Refer to the yellow and orange circles for the existing implementation.  All others represent future phases.\n",
    " \n",
    "\n",
    "\n",
    "7.5 Reporting\n",
    "Power BI will be the primary visualization tool used for developing reports against the semantic model. Templates are developed to support rapid report development and provide Amynta Group with a more branded appeal to the report consumers. \n",
    "\n",
    "7.5.1 Legacy Production Reports\n",
    "The following reports are sample Production or Product dashboard reports that are being used as reference reports for the initial Product Dashboard report being built in the Lakehouse environment.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    " \n",
    "\n",
    "7.5.2 New Product Dashboard Reporting\n",
    " \n",
    "Product Submission Overview\n",
    "\n",
    " \n",
    "Product Submission Premiums\n",
    " \n",
    "Product Submissions Bound vs Unbound\n",
    "\n",
    "7.5.3 Operational Data Platform Health\n",
    "The following reports were created to track the operational health of the solution.  These reports will provide details on pipeline runs at all levels of the data architecture.\n",
    "\n",
    " \n",
    "Pipeline Operational Health Dashboard\n",
    " \n",
    "Pipeline Object Execution Details\n",
    "\n",
    "\n",
    " \n",
    "Pipeline Execution Details\n",
    "\n",
    "7.6 Analytics Services \n",
    "Fabric Semantic Models will be used to support the semantic layer. Direct-Lake Semantic Models are used as the primary semantic model for standard reports. For this phase, 1 semantic model will be developed to support all reporting. This model leverages the Gold Lakehouse and all Gold layer tables will be included in the semantic model.  The following model and calculations group has been created to support ADS-related reporting for phase 1.\n",
    "\n",
    " \n",
    "Product Details Semantic Model\n",
    "\n",
    " \n",
    "Calculation Groups are used to easily toggle the period being analyzed on the front end while reducing excess measure development and providing context for aggregates.\n",
    "\n",
    "An additional model was built to support reporting for the operation of the solution.  This model will provide operational health from ingestion through the gold layer of the data architecture.\n",
    " \n",
    "\n",
    " \n",
    "Calculation groups for the operational model.\n",
    "\n",
    "\n",
    "NOTE: Additional semantic models will be created to support ad-hoc and canned reporting requirements.\n",
    "\n",
    "8. DevOps Integration\n",
    "8.1 Source Control Management \n",
    "The Development Process will leverage Azure DevOps and be fully integrated into the environment named workspaces.  Leveraging Fabric Deployment Pipelines for moving code and artifacts from DEV through to PROD environments.  For this project infrastructure external to Fabric, such as Key Vault, is managed through manual operations. Production Reporting follows these guidelines: \n",
    " \n",
    "\n",
    "•\tFeature/User DEV: Iterative development environment where engineers test code on branches. No guarantees are made about reliability in this environment without consistent branching. Developers are responsible for keeping their branches in sync with the Main branch/workspace, in their Feature Workspace. Developers will submit pull requests (PR’s) to merge code into the Main branch.\n",
    "•\tDEV: Development workspace where new code and changes will be unit tested by developers before promoting to UAT. This is the primary workspace for the Main branch.\n",
    "•\tUAT: Test workspace that mimics dev or production as closely as possible and is used for a longer observation and testing period. \n",
    "o\tPerformance requirements: Architect will run unit tests during this phase for every change to ensure pipeline runtime and reports meet performance expectations.\n",
    "•\tPRD: The most stable environment with rigorous processes defined for deployment, rollbacks, monitoring, and alerting. Code should be promoted here pending code reviews, UAT sign-off, and all approvals are completed. \n",
    "8.2 Code Deployment and Management Processes\n",
    "At the end of each sprint the lead architect will test the DEV environment and ensure all features that are complete are merged to DEV. Once this unit testing and validation is complete, which includes Fabric Capacity Metrics analysis, the client team will notify the lead and the stakeholders that UAT has been updated with changes that the client team will specify to the stakeholder upon notifying them. \n",
    "8.3 Backward Compatibility Strategies\n",
    "Azure DevOps facilitates branch reversion for quick rollbacks. For data recovery, Delta Time Travel/Restore is employed to revert datasets to a specific point in time, matching the branch timestamp, and ensuring minimal disruption. This combination allows efficient and reliable recovery of both code and data in the event of an issue.\n",
    "\n",
    "9. Cloud Fundamentals/Readiness Recommendations\n",
    "Fog Solutions was not responsible for the Azure Landing zone and since the data for Production Reporting is not sensitive or requires additional security, Amynta Group elected to wait to implement a true Azure Landing Zone architecture, but Fog made the following recommendations.\n",
    "\n",
    "9.1 Azure Region\n",
    "Amynta Group has a large datacenter footprint in Azure (over 60% of North American infrastructure is hosted in Azure). Comprised mostly of IaaS services, multiple PaaS and SaaS offerings are used. Amynta Group has some HA configurations in IaaS and is familiar with the secondary data center footprint. With that in mind, Amynta Group is already using the East US data center for the primary data center and West US for the secondary data center where HA configurations are implemented. \n",
    "Amynta Group hosts this large footprint inside of 1 large production subscription. This is their enterprise standard for resource organization.\n",
    "On-premises data centers, connectivity providers and routing services are configured with the East US and West US regions. \n",
    "\n",
    "Fog Recommendation:\n",
    "•\tImplement pay as you go subscription for hosting non-production Fabric Capacities. \n",
    "\n",
    "9.2 Networking\n",
    "For this Fabric implementation, private endpoints and virtual networks were not implemented. There are two reasons for this:\n",
    "1.\tAmynta currently has Power BI embedded applications.  This is due to Power BI not supporting private link capabilities today.  This will be addressed in a future Fabric release when they allow per use space private links.\n",
    "2.\tThis is due to the data classification as enterprise but not sensitive and for budget reasons, Amynta Group elected to do this at a later phase once there is more adoption for the Production Reporting solution.  Also, once the data sources have highly classified or PII/HIPAA regulated data elements or Amynta Group trade secrets. \n",
    "\n",
    "We leveraged a Fabric Gateway on an existing private Azure VNET, for extraction pipelines to connect directly to IMS SQL Server databases and eventually this same gateway will be used for connectivity to ServiceNow databases.\n",
    "\n",
    "Fog Recommendation:\n",
    "•\tImplement Azure Landing Zone pattern, hub-spoke architecture (https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/traditional-azure-networking-topology)\n",
    "•\tDeploy Fabric private endpoints as spoke networks to the hub (https://learn.microsoft.com/en-us/fabric/security/security-managed-private-endpoints-overview)\n",
    "•\tDeploy Fabric to managed VNET (https://learn.microsoft.com/en-us/fabric/security/security-managed-vnets-fabric-overview)\n",
    "\n",
    "\n",
    "\n",
    "9.3 Identity and Role-Based Access Control\n",
    "Amynta Group has an existing and robust access control process, where organizational Entra users and groups only, via Entra ID Premium and Privileged Identity Management is used for all Azure management and administrative duties. The custom Fabric Capacity Administrator role is an example of a PIM enabled role, for assigning capacity or for changing the capacity SKU. This adheres to the established Amynta Group login policies for Fabric and the Azure portal, in the same way Power BI is accessed organizationally today. This approach ensures that access controls are consistent and compliant with organizational security standards.\n",
    "\n",
    "No PII or HIPAA data is used or identified for the Production Reporting initiative. \n",
    "\n",
    "Fog Recommendation:\n",
    "•\tImplement Microsoft Purview for classifying sensitive data (https://learn.microsoft.com/en-us/purview/information-protection)\n",
    "•\tImplement Data Loss Prevention (DLP) with Microsoft Purview and Azure Compliance Center (https://learn.microsoft.com/en-us/purview/dlp-learn-about-dlp)\n",
    "\n",
    "\n",
    "9.4 Cost Optimization and Cost Governance\n",
    "For each deployment to UAT, prior to the deployment the lead architect and client architect will perform a capacity analysis review together. If there are processes that are utilizing a significant number of compute-units (CU’s) the architects will review the problem area and meet with the authoring developer to remediate and adjust.\n",
    "Each month, a Cost assessment will be performed manually from the Azure Cost Management portal with the architects and project stakeholder owners to ensure capacity and cost are not exceeding expectations. \n",
    "\n",
    "Fog Recommendation:\n",
    "•\tImplement Azure Budget for each capacity \n",
    "•\tAs adoption and usage increases, scale out to multiple departmental capacities and regional capacities.\n",
    "•\tAs adoption and usage increases, leverage Capacity Reservations (https://learn.microsoft.com/en-us/azure/cost-management-billing/reservations/fabric-capacity)\n",
    "\n",
    "9.5 High Availability/Disaster Recovery\n",
    "For this phase, primarily to minimize costs, Amynta Group has elected not to enable the Business Continuity and Disaster Recovery (BCDR) setting for the Fabric capacity, they are aware of this and understand the risks. As more continents come online and capacity grows, this will be revisited regularly.\n",
    "\n",
    "Fog Recommendation:\n",
    "•\tTurn on BCDR switch for Fabric Capacity, treat the Production Reporting workspaces as East US/West US failover operations in the broader Amynta Group infrastructure portfolio (https://learn.microsoft.com/en-us/fabric/onelake/onelake-disaster-recovery#disaster-recovery)\n",
    "\n",
    "\n",
    "9.6 Compliance\n",
    "For this phase, there are no HIPAA, GDPR, or PII data elements. However, the HIPAA and GDPR policies were confirmed as implemented in the Azure Trust Center for the subscription. This is one of the reasons Amynta Group uses a single subscription for managing all their infrastructure is due to compliance policies and other infrastructure policies.\n",
    "\n",
    "\n",
    "9.6 Monitoring and Alerting\n",
    "For this phase, Amynta Group elected not to configure Azure Log Analytics/Azure Monitor for the Fabric event log. For this phase, the team used the Monitoring tab in Microsoft Fabric to view scheduled operations manually and troubleshoot errors from there. Fabric pipelines leverage Outlook email alerts for failure notifications.  Artifact Monitoring is done through the built in Fabric Monitor and through the entire development team having access to the Fabric Capacity Metrics App. During unit testing, each developer will test their process in DEV while monitoring capacity and in UAT to ensure that resource utilization is on par.  \n",
    "\n",
    "Fog Recommendations:\n",
    "•\tImplement and configure Log Analytics/Azure Monitor with alerting, for the Fabric Capacity and Fabric Workspaces to track activities (https://learn.microsoft.com/en-us/fabric/admin/track-user-activities)\n",
    "•\tAs adoption increases, implement a Fabric/Power BI model for logging data (https://learn.microsoft.com/en-us/power-bi/connect-data/create-dataset-log-analytics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gpt_researcher import GPTResearcher\n",
    "from gpt_researcher.utils.enum import ReportType, ReportSource, Tone\n",
    "\n",
    "new_vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "sections = [ \"Project Summary\", \"Scope of the Solution\", \"Stakeholders and Audiences\", \"Solution Design Overview\", \"User Roles and Security Considerations\", \"Definition of User Roles\", \"Data Sources and Ingestion\", \"Data Ingestion Strategy\", \"Data Storage and Management\", \"Storage Solutions\", \"Data Retention and Archival Policies\", \"Data Encryption and Data Security\", \"Encryption Methods\", \"Data Security Techniques\", \"Analytics and Reporting\", \"Reporting\", \"Analytics Services\", \"DevOps Integration\", \"Source Control Management\", \"Code Deployment and Management Processes\", \"Rollback Strategies\", \"Cloud Fundamentals/Readiness\", \"Regional Planning and Data Center Utilization\", \"Network and Infrastructure Setup\", \"Azure Resources\", \"Network\", \"Identity and Access Management\", \"Cost Optimization and Governance\", \"Backup and Recovery Solutions\", \"High Availability/Disaster Recovery Plan\", \"Monitoring and Alert Systems\", \"Appendix and References\", \"Glossary of Terms\", \"Reference Documents\"]\n",
    "\n",
    "research_query = f\"\"\"\n",
    "You are to create a design document that describes, in detail, a solutions design for Assured Partners Broker in a box DataFabric.  It is imperative the design document is detailed and provides a comprehensive overview of DataFabric implemntation, not the application broker in a box.  Focus HEAVILY and entirely on data fabric.\n",
    "\n",
    "Use the sections provided to create a detailed report.  Each section should be expanded to include as much detail as possible.  Use the internet to expand on recommended practices.\n",
    "\n",
    "Be verbose and expand and topics as much as possible.  If the vector_store provided doesn't have informmation, state would should be in each sub topic provided.\n",
    "\n",
    "Do not mention anything about Amynta, but assume the Broker in the box data fabric implmentation is the same.\n",
    "\n",
    "Use the internet to expand on recommended practices.\n",
    "\n",
    "Use this example report as a guide.  Replace content in the example with what you can find in the vector_store.  If no information can be found for broker in a box, or a section, elaborate on best practices and what *should* be in the report and how it should be designed.\n",
    "\n",
    "IT IS IMPERATIVE THAT YOU DO NOT REFERENCE AMYNTA AT ALL.  ONLY REFERENCE ASSURED PARTNERS and the product \"broker in a box\"\n",
    "\n",
    "BE VERBOSE.  The report should be at least 10 pages long.  Each section should be 2 paragraphs.  Each section should have a lead-in.  Each sub section should be 2 paragraphs.  If you can't find information, elaborate on what should be in the report.\n",
    "\n",
    "subtopics should be {\",\".join(sections)}\n",
    "\n",
    "{amynta_report}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "researcher = GPTResearcher(\n",
    "    query=research_query, \n",
    "    report_type=ReportType.ResearchReport, \n",
    "    report_format=\"markdown\", \n",
    "    report_source=\"langchain_vectorstore\", \n",
    "    tone=Tone.Descriptive, \n",
    "    max_subtopics=100,\n",
    "    source_urls=None, \n",
    "    document_urls=None,\n",
    "    vector_store=vector_store,\n",
    "    query_domains=[])\n",
    "\n",
    "research_result = await researcher.conduct_research()\n",
    "\n",
    "report = await researcher.write_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written to outputs/./assured_partners_data_fabric_sources.docx.docx\n",
      "<coroutine object GPTResearcher.write_introduction at 0x3fa97e340>\n"
     ]
    }
   ],
   "source": [
    "# #await researcher.write_introduction()\n",
    "# introduction = researcher.write_introduction()\n",
    "\n",
    "from backend.utils import write_md_to_word\n",
    "result = await write_md_to_word(report, \"./assured_partners_data_fabric_sources.docx\")\n",
    "print(introduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create langchain retreiver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate by section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.bing_search import BingSearchResults\n",
    "from langchain_community.utilities import BingSearchAPIWrapper\n",
    "from langchain.agents import AgentExecutor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.tools import tool\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Specify the path to your local model or the model name from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Initialize the Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "\n",
    "new_vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = new_vector_store.as_retriever()\n",
    "\n",
    "api_wrapper = BingSearchAPIWrapper(bing_subscription_key=os.getenv(\"BING_API_KEY\"))\n",
    "bing_tool = BingSearchResults(api_wrapper=api_wrapper)\n",
    "new_vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = new_vector_store.as_retriever()\n",
    "\n",
    "@tool\n",
    "def ap_data(search_query):\n",
    "    \"\"\"\n",
    "        Retriever for Assured Partners Broker in a Box.  Use to find any information about assured partners and broker in a box.\n",
    "    \"\"\"\n",
    "    return new_vector_store.as_retriever().invoke(search_query)\n",
    "\n",
    "# retriever_tool = create_retriever_tool(retriever=new_vector_store.as_retriever(), name=\"assured_partners_borker_in_a_box_retriever\", description=\"Retriever for Assured Partners Broker in a Box.  Use to find any information about assured partners and broker in a box.\")\n",
    "\n",
    "os.environ[\"BING_SEARCH_URL\"] = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "\n",
    "from langchain_openai import AzureOpenAI, AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "deployment_name = 'gpt-4o' #os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "model_name = 'gpt-4o' #os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    deployment_name=deployment_name,\n",
    "    model_name=model_name,\n",
    "    base_url=os.getenv(\"AZURE_OPENAI_BASE_URL\")\n",
    ")\n",
    "\n",
    "tools = [bing_tool, ap_data]\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate existing file and generate with vector_store + internet + prompts for each Heading and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing section: 1. Project Summary\n",
      "Processing section: Scope of the Solution\n",
      "Processing section: Stakeholders and Audiences\n",
      "Processing section: 2. Solution Design Overview\n",
      "Processing section: 2.1 Solution Objective\n",
      "Processing section: 2.2 Current State Solution Diagram\n",
      "Processing section: 2.3 Future State Solution Architecture\n",
      "Processing section: 3. User Roles and Security Considerations\n",
      "Processing section: 3.1 Definition of User Roles\n",
      "Processing section: 4. Data Sources and Ingestion\n",
      "Processing section: 4.1 Identification of Data Sources\n",
      "Processing section: 4.2 Data Ingestion Strategy\n",
      "Processing section: 4.3 Data Ingestion Metadata Framework\n",
      "Processing section: Ingestion Metadata \n",
      "Processing section: 4.3.1.1 Source Table  \n",
      "Processing section: 4.3.1.2 Source Object Table\n",
      "Processing section: 4.3.1.3 Object Fields Table\n",
      "Processing section: 4.3.2 Data Quality Metadata\n",
      "Processing section: 5. Data Storage and Management\n",
      "Processing section: 5.1 Storage Solutions \n",
      "Processing section: 5.2 Data Organization and Structuring\n",
      "Processing section: 5.3 Common Data Model\n",
      "Processing section: 6. Data Encryption and Security\n",
      "Processing section: Encryption Methods \n",
      "Processing section: 6.2 Data Security Techniques\n",
      "Processing section: 7. Analytics and Reporting \n",
      "Processing section: 7.1 Analytics Platform\n",
      "Processing section: 7.3 Fabric Domains\n",
      "Processing section: 7.4 Fabric Workspaces\n",
      "Processing section: 7.5 Reporting\n",
      "Processing section: 7.5.1 Legacy Production Reports\n",
      "Processing section: 7.5.2 New Product Dashboard Reporting\n",
      "Processing section: 7.5.3 Operational Data Platform Health\n",
      "Processing section: 7.6 Analytics Services \n",
      "Processing section: 8. DevOps Integration\n",
      "Processing section: 8.1 Source Control Management \n",
      "Processing section: 8.2 Code Deployment and Management Processes\n",
      "Processing section: 8.3 Backward Compatibility Strategies\n",
      "Processing section: 9. Cloud Fundamentals/Readiness Recommendations\n",
      "Processing section: 9.1 Azure Region\n",
      "Processing section: 9.2 Networking\n",
      "Processing section: 9.3 Identity and Role-Based Access Control\n",
      "Processing section: 9.4 Cost Optimization and Cost Governance\n",
      "Processing section: 9.5 High Availability/Disaster Recovery\n",
      "Processing section: 9.6 Compliance\n",
      "Processing section: 9.6 Monitoring and Alerting\n",
      "{'messages': [HumanMessage(content='\\nYou are to create a design document that describes, in detail, a solutions design for Assured Partners Broker in a box DataFabric.  It is imperative the design document is detailed and provides a comprehensive overview of DataFabric implemntation, not the application broker in a box.  Focus HEAVILY and entirely on data fabric.\\n\\nUse the sections provided to create a detailed report.  Each section should be expanded to include as much detail as possible.  Use the internet to expand on recommended practices.\\n\\nBe verbose and expand and topics as much as possible.  If the vector_store provided doesn\\'t have informmation, state would should be in each sub topic provided.\\n\\nDo not mention anything about Amynta, but assume the Broker in the box data fabric implmentation is the same.\\n\\nUse the internet to expand on recommended practices.\\n\\nUse this example report as a guide.  Replace content in the example with what you can find in the vector_store.  If no information can be found for broker in a box, or a section, elaborate on best practices and what *should* be in the report and how it should be designed.\\n\\nIT IS IMPERATIVE THAT YOU DO NOT REFERENCE AMYNTA AT ALL.  ONLY REFERENCE ASSURED PARTNERS and the product \"broker in a box\"\\n\\nBE VERBOSE.  The report should be at least 10 pages long.  Each section should be 2 paragraphs.  Each section should have a lead-in.  Each sub section should be 2 paragraphs.  If you can\\'t find information, elaborate on what should be in the report.\\n\\nsubtopics should be Project Summary,Scope of the Solution,Stakeholders and Audiences,Solution Design Overview,User Roles and Security Considerations,Definition of User Roles,Data Sources and Ingestion,Data Ingestion Strategy,Data Storage and Management,Storage Solutions,Data Retention and Archival Policies,Data Encryption and Data Security,Encryption Methods,Data Security Techniques,Analytics and Reporting,Reporting,Analytics Services,DevOps Integration,Source Control Management,Code Deployment and Management Processes,Rollback Strategies,Cloud Fundamentals/Readiness,Regional Planning and Data Center Utilization,Network and Infrastructure Setup,Azure Resources,Network,Identity and Access Management,Cost Optimization and Governance,Backup and Recovery Solutions,High Availability/Disaster Recovery Plan,Monitoring and Alert Systems,Appendix and References,Glossary of Terms,Reference Documents\\n\\n\\n\\nSolution Design\\n\\n\\u2003\\n1. Project Summary\\nThe Amynta Group, an industry-leading team of warranty and specialty risk companies, as well as managing general agents, has experienced significant growth since its founding in 2018. Through strategic acquisitions, Amynta has expanded its warranty and services offerings across the automotive, consumer products, and specialty equipment industries. To support this growth and enhance business visibility into data such as policies, actuarial, and claims, Amynta requires an enterprise data strategy that provides a unified and scalable data platform. This will enhance data accessibility, governance, and analytics across the organization. Amynta has been exploring Microsoft Fabric as the foundation for this data strategy.  Ultimately, Amynta aims to make reporting and analytics accessible across the organization. By implementing a Microsoft Fabric-based solution, Amynta will be able to achieve this goal while centralizing data management, ensuring a single source of truth, and reducing data silos. \\n\\nThe central goal of this initiative is to furnish the Microsoft Fabric platform, which is intended to support a consolidated enterprise-wide Lakehouse environment.  The Lakehouse will serve as the trusted foundation for analytic capabilities like BI, ML, and AI.  It includes core components for ingesting, transforming, and managing data for the purpose of supporting downstream analytics.  \\n\\nScope of the Solution\\n1.\\tFabric environment is provisioned with public endpoints, and OneLake is leveraged as the foundation of the solution and for future business use case scalability and diversity \\n2.\\tData will be ingested and processed daily for operational purposes and processed at month end which is the required SLA. \\n3.\\tSolution will leverage a single Direct-Lake semantic model and Power BI reports will be as up to date as their underlying delta tables.\\n4.\\tStar schema that combines device data and inventory data will be created to ensure the success of streamlined report development\\n\\nStakeholders and Audiences\\nThe primary stakeholders for this solution are:\\n•\\tChief Financial Officer – Monthly and Quarterly Business Reviews\\n•\\t36 Business Unit Leaders – used for Monthly and Quarterly Business Reviews and also use it daily for operational needs.   NOTE: Initial phase will only focus on the Amynta Dealer Services BU.\\n•\\tDirect reports to Business Unit Leaders as needed – used for operational needs\\n•\\tActuarial Team – used to support risk management across the businesses\\n2. Solution Design Overview\\n2.1 Solution Objective\\nCurrently, Amynta has a diverse set of technologies to meet the varying needs of its business analytics across different business units. Some teams prefer using Excel for its ease of use, flexibility, and data manipulation features, enabling quick insights from smaller datasets. In contrast, other business units integrate Power BI with Microsoft Synapse, allowing for advanced data modeling, large-scale data processing, and real-time analytics to support large and diverse datasets. This combination supports more complex, enterprise-wide reporting and predictive analysis. Additionally, other business units are leveraging SQL Server to manage their datasets and Power BI to create dynamic dashboards and interactive reports. In summary, there’s a mix of analytic support models that are serving the business analytics needs.  At the same time, there is repetitive work being done, data is highly siloed, and business users are spending time building and managing these environments.  With all of this in mind, Amynta wants to build a consolidated Lakehouse environment to provide value to the organization\\n\\nTechnical success metrics defined by Amynta Group:\\n•\\tPrecision and Reliability in data collection \\n•\\tIntegration of multiple sources into a unified platform on a regular basis\\n•\\tReduction of manual extraction and processing of data\\n•\\tPerformance: Reports should be rendered in under 30 seconds for an end user\\n•\\tPerformance: Pipeline refresh should occur in under 1 hour\\n•\\tOperational awareness of data platform activities\\n•\\tScheduled refresh and delivery of executive reports\\n\\nBusiness success metrics defined by Amynta Group:\\n•\\tEfficiency gains in policy renewal\\n•\\tInsights achieved in product management\\n•\\tDetecting low performing agents \\n•\\tReducing time from submission to quotes to binds\\n\\n2.2 Current State Solution Diagram\\nThe following diagram depicts the current state of the Analytics team that exists within the Actuarial business.  It follows a legacy enterprise data warehouse pattern.  Currently, only two sources (Guardsman and WarranTech) are being brought into this environment.\\n\\n\\n\\n\\n\\n \\n\\n\\nData for Guardsman is being replicated into the Synapse environment today using Qlik Replicate.  The team is looking to retire the Qlik solution.\\n\\nData for WarranTech is pulled into the Synapse environment three times per day.  It is being ingested using Azure Data Factory.\\n\\n2.3 Future State Solution Architecture\\nThis is the diagram of the Fabric solution with the 3 sources:\\n \\n\\n1.\\tThe pipeline will run and execute a Notebook, this Notebook is responsible for ingesting data from ALIS into the Bronze Lakehouse artifact as a queryable Delta Table. \\n2.\\tThe pipeline will execute a Copy Activity that leverages an On-Premises gateway to extract data from the IMS or Amynta Dealer Services data from the SQL Server source.\\n3.\\tThe Fabric Data Factory Pipeline will be responsible from orchestration from this point forward, it will be stored in the Fabric Workspace, but the diagram is to depict its responsibilities and show that secrets and config values will be stored in Key Vault.\\n4.\\tThe pipeline will execute a transformation step using Dataflow Gen2/Pipeline/Stored Procedure/Notebook to transform the data to support the reporting layer and semantic model.\\n5.\\tReports are generated off a separate Direct-Lake semantic model. The initial report to support phase 1 is the Product Dashboard report, which will be built off a semantic model.\\n6.\\tThe following Azure Resources will be utilized for the project:\\na.\\tAzure Key Vault - used for storing configurations and secrets for the pipeline and notebooks.  API credentials to Logic Monitor are an example of this type of configuration\\nb.\\tFabric Capacity - a dedicated capacity will be provisioned and attached to the workspaces we will use during the project, the ONLY Fabric specific abilities in this Azure Resource are scaling SKU up and down, and starting and stopping the capacity\\nc.\\tPower Automate Flow - used to start and stop the Fabric Capacity Resource so that Amynta can recognize further cost savings when using CoPilot SKUs\\n\\t\\t\\n\\n3. User Roles and Security Considerations\\nMicrosoft Fabric supports a range of security measures to control what features of the workspace users have access to, and who can read or modify what data. \\nIn general, the following are recommended: \\n•\\tLeverage the medallion architecture to curate and combine common data across varying sources, securing access to roles that need access to each medallion layer, via separate Lakehouses\\n•\\tUse of Azure Key Vault for securing credentials that Pipelines and Notebooks via Entra Service Principal/Workspace Identity\\n•\\tUse of Workspace Access Control to control who can view or run which notebooks and view data in the Gold layer \\n•\\tLeverage Direct-Lake for handling varying source refresh times using Row-Level Security (RLS) for varying departmental data security \\n\\n3.1 Definition of User Roles\\nSpecific roles will be leveraged by configuring Entra Groups to each role. The following roles and groups will be leveraged for the implementation:\\nResource Group Contributor Group \\n•\\tCan provision Fabric Capacity as an Azure Resource \\nCustom Fabric Capacity Admin Group \\n•\\tCan create & configure workspaces to use a specific capacity \\n•\\tCan scale capacity performance SKUs/tiers \\n•\\tMust use PIM to enable this for scaling SKU up or down and for workspace assignment\\n•\\tCustom definition:\\n•\\t\"Microsoft.Fabric/capacities/read\"\\n•\\t\"Microsoft.Fabric/capacities/write\"\\n•\\t\"Microsoft.Fabric/capacities/pause/action\"\\n•\\t\"Microsoft.Fabric/capacities/resume/action\"\\n•\\t\"Microsoft.Fabric/workspaces/read\"\\n•\\t\"Microsoft.Fabric/workspaces/write\"\\n•\\t\"Microsoft.Fabric/workspaces/assign/action\"\\n•\\t\"Microsoft.Fabric/operations/read\"\\nWorkspace Admin Group \\n•\\tResource management, create, modify, and delete resources \\n•\\tUser and Access Control \\nWorkspace Contributor Group \\n•\\tMost developers have this role \\n•\\tResource management, create, modify, and delete resources \\nWorkspace Viewer Group \\n•\\tConsume and view all content inside of the workspace but cannot, create, modify, or delete artifacts \\n•\\tReporting access to view reports\\n•\\tRole-Based Access Control (RBAC) Strategy\\n4. Data Sources and Ingestion\\n4.1 Identification of Data Sources\\nFor this phase of the project, we have strategically selected two data sources that offer the most significant value for our investment. These choices are based on their potential to maximize efficiency and impact, ensuring optimal resource utilization and return. These sources are:\\no\\tALIS Policy Administration:\\no\\tOverview - ALIS is a comprehensive, cloud-based software solution designed primarily for Managing General Agents (MGAs), Managing General Underwriters (MGUs), wholesalers, and program administrators in the insurance industry.  ALIS is going to be the standard for policy administration across Amynta.  Currently, only a handful of business units use the platform.  \\no\\tData Access – ALIS is a SaaS solution and provides several APIs to access the policy data.  The ALIS API uses token-based authentication, with each token linked to a user account, enabling granular permission management for integrations.\\no\\tDatatype – JSON\\no\\tData Ingest Frequency – Daily\\no\\tExpected Volumes – \\no\\tHistorical:  < 500 GB total with historical data\\no\\tIncremental:  < 2-5 GB total with historical data.  Will accelerate as more business units join the platform.  Roadmap is being determined.\\n\\no\\tIMS Insurance Management System:\\no\\tOverview - Designed specifically for MGAs, program administrators and carriers, Insurance Management System (IMS) is a full-featured, scalable policy administration system. IMS includes everything you need for end-to-end policy lifecycle management, from rating, insured tracking and clearance and company and market management.  This is a legacy system currently in use with a few business units and will ultimately be replaced by ALIS.\\no\\tDatatype – Structured data from a SQL Server source\\no\\tData Ingest Frequency – Daily\\no\\tExpected Volumes –\\no\\tHistorical: < 50 GB\\no\\tIncremental: <200 MB\\n\\no\\tFuture Data Sources\\nThe following table shows the list of data sources in priority order that will be leveraged throughout the various phases of the engagement.\\nSource Systems\\tAverage Amount of Reporting from Source\\nWarranTech\\t40%\\nGuardsman\\t35%\\nTPA\\t35%\\n\\t\\nAll sources are internet accessible API extracts with an Amynta Group token, CSVs, or SQL-based sources. The token is managed by Azure Key Vault for secure access.\\n4.2 Data Ingestion Strategy\\nData ingestion is performed through Fabric Data Factory for orchestration of copy data pipelines and notebooks. Each source has its own orchestration pipeline so it can run as a stand-alone process but there is also a master orchestration pipeline that runs one time per day that executes each child pipeline for each source. For the sources in this current phase:\\n•\\tALIS – this API call to ingest the raw data is implemented in a Spark Notebook inside of Fabric. Since the API call is a more complex source with specific vendor requirements on how to pull the data, a Notebook is used.\\n•\\tIMS – this data is extracted over a Fabric Gateway and using Fabric Data Factory and copy activity pipelines to securely access the SQL Server data hosted on a private virtual network and Azure VM.  \\nWithin each pipeline and notebook in the ingestion layer, a metadata framework is leveraged that uses pipeline variables (json) and Lakehouse control tables, in the Utility Lakehouse. There are 2 key phases of data ingestion in the medallion architecture, these are: \\n1.\\tSource System to Raw: This part of the solution is responsible for extracting the raw data from the source systems into the Files location of the Bronze Fabric Lakehouse. These extracts are done in full and incrementally depending on which source, but these Files are stored in the Files section with a timestamp path and the source system. Example: \\nIMS /2024/11/27 – for this project, the files on the same day are overwritten \\n2.\\tRaw to Bronze: In the Bronze layer the data is simply ingested to the ʻBronze Delta tablesʼ “as-is” along with audit columns like ingestion date, batch ID, etc. Incoming data is composed of parquet files for approximately 500 tables daily. The primary purpose of bronze tables is the ability to provide a historical archive of source data lineage, auditability, and reprocessing if needed without rereading the data from the source system.  \\n4.3 Data Ingestion Metadata Framework\\nAmynta has numerous data sources containing various types of data stored in various ways. All of these data share a set of common attributes: data location, data format, can I identify new or changed data, if yes to last how do I identify the changed data, what are the keys to update for changed data. \\n \\nIngesting this data into a common data repository is also a standard process: connect to the source, copy the source, write the source to bronze, merge the source to Silver. \\nWhile this is an oversimplification, it is meant to demonstrate the opportunity to control the data ingestion process through a metadata framework. \\n\\n4.3.1\\tIngestion Metadata \\nThe ingestion metadata is driven by 3 primary tables:\\u202f \\n•\\tSource – connection info for the source\\u202f \\n•\\tSource_Object – details required for processing object (PKs, Incremental attributes)\\u202f \\n•\\tObject_Field – data type, column order, nullable or not\\u202f \\n\\n4.3.1.1 Source Table\\u202f \\n \\u202f \\nThis table will have one row for each specific source instance.\\u202f \\n\\u202f \\nThe bulk of the information is carried within the source_connection_json column.\\u202f The reason to store it in JSON is to prevent this table from constantly having columns added to support new source platforms. For example, for ADLS as a source, the table would need to have added Storage Account, Storage Container, StoragePath; but a SQL Server source would need Servername, DBName, UserId, Password (keyvault secret name).\\u202f \\n\\nContinually adding new source platforms leads to more columns or often misuse of existing columns out of convenience (i.e. a source type doesn’t need DBName for this SourceType so developers may store a different value here out of convenience that the new source type does need).\\u202f \\n \\nHere is an example of source_connection_json:\\u202f \\n{\\u202f\\u202f\\u202f \"development\": { \\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \"server_name\": \"fogdemoserver.database.windows.net\", \\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \"user_id\":\"FabricLoader\", \\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \"password\":\"sc-importer-password\", \\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \"db_name\":\"abc_trans_db\" \\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \\u202f },\\u202f\\u202f \\n\\u202f\\u202f\\u202f \"production\": { \\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \"server_name\": \"fogdemoserver.database.windows.net\", \\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \"userId\":\"FabricLoader\", \\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \"password\": \"sc-importer-password\", \\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \"db_name\":\"abc_trans_db\" }\\u202f \\n\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f\\u202f \\u202f }\\u202f \\n\\u202f \\nWithin source_connection_json there must always be a minimum of one value kept in a KeyVault Secret (or secure equivalent).\\u202f \\n\\u202f \\nDepending on the source type there will be different name/value pairs.\\u202f The JSON also allows storage of connections for one to many environments.\\u202f Every environment has its own metadata database. Within every MetaData database there is a function called fn_Environment()\\u202fwhich is hard coded to return “Development”, “Production” or other required environments.\\u202f \\n \\nThis function will be restricted to database admins. The Fabric Managed Identity will have rights explicitly denied to ‘alter the function’.\\u202f This is to prevent accidental changes during deployment or mistakes during development.\\u202f \\n \\n\\u202fWhen a Pipeline or a Notebook needs Source Connection Information, they will call a stored procedure passing in the Source Name and will get the connection information or for pipelines it will return Connection Info and the Objects to be processed.\\u202f \\n\\n4.3.1.2 Source Object Table\\nThe source_object table identifies the specific objects/tables to be loaded for a source.\\u202f \\n\\n \\n\\n \\n\\nThe source_object table identifies the specific objects/tables to be loaded for a source.\\u202f \\n \\nIf source type is a database there will be an object_name and schema_name. If the source is an ADLS storage account with a Path in connection properties, then object_name could be a folder name and object_schema would be blank.\\u202f \\n\\u202f \\nThe load_grouping column can be used to differentiate into batches.\\u202f For a source, maybe five tables can start loading at 9:00 pm but the other eighty tables we need to wait for 3:00 am.\\u202f A trigger could execute a pipeline to do source and load_grouping 1, then run a separate pipeline later with load_grouping of 2.\\u202f \\n \\nThe next three columns relate to incremental loads.\\u202f Wherever possible is_incremental should be set to “1”, as a “0” would mean full load which is basically a truncate target and reload (impractical for large tables).\\u202f \\n \\nThe watermark_column drives the incremental load, it is added to criteria for the query that is generated.\\u202f Some metadata tables store the max value of the watermark_column, a better practice is to query the target for the max date when incremental ingestion is starting; this handles situations where the last load had to be deleted out of target because of some issue.\\u202f \\n \\nWhenever an incremental load is run, the new data is brought to Bronze (where day after day there may be new versions of the record).\\u202f Silver only has the most current version of the record.\\u202f To process an Upsert into Silver the merge process must have columns that identify new records or updated records when compared to current Silver.\\u202f \\n \\nThe second half of the table shown in the 2nd diagram starts with three custom columns. The ingest process will automatically generate a SELECT * FROM source WHERE watermark_column > ‘2024/12/3 03:42:472’.\\u202f If custom logic is required, adding that SQL to custom_query will override the auto created statement.\\u202f \\nGenerally, ingestion brings down all columns with no transformation. However, a very wide table might require the process to bring 10 columns not 500. Or eliminating blobs that are stored on the object. \\u202f \\n\\u202f \\nThe copy from source uses pipelines based on the source type.\\u202f Using metadata so that if we have ten SQL Servers as sources, they can all use a common pipeline.\\u202f The custom_pipeline column allows the generic pipeline to be overridden and have a different pipeline called. Possibly a very complex REST API might need their own custom pipeline.\\u202f \\n\\nProcessing data from Bronze to Silver can also be done by a generic notebook; given that once they have landed in Bronze all data is in same format (Lakehouse delta tables).\\u202f If custom processing is required a specific notebook can be named.\\u202f \\n\\u202f \\nThe is_active if “0” means object will not be processed.\\u202f\\n\\n4.3.1.3 Object Fields Table\\nThe object_fields table is associated with to a source_object. The source_id is carried for convenience but is not required since you could join through source_object.\\u202f \\n \\n\\nThis table contains basic column information which allows the process Bronze to Silver to create the Silver target table if it is not there. If the table is present, it allows a check to see if there have been schema changes and add new columns to existing table.\\u202f\\n\\n4.3.2 Data Quality Metadata\\nFor the initial project some very simple data quality is being implemented.\\u202f In future phases, these tables may be extended to support other data quality checks or possible future integration Purview and MDM integration may negate the need for these tables at all.\\u202f \\n \\nData quality can be applied between Bronze and Silver or while loading Gold. \\n \\nThere are two tables. The dq_object_field_table which lists a specific source object column and a data quality validation check or cleanse to apply.  The second table is the dq_field_lookup table which carries reference information to support application of rules and cleanse. \\n\\ndq_object_field\\u202fcolumns \\ndq_object_field_id \\tprimary key – surrogate key auto generated \\nobject_field_id \\t foreign key identifies column this record applies to \\nfail_on_null \\t if the value for the column is NULL then fail \\nperform_lookup \\t do a lookup and if current value of column is found, replace with the found lookup value \\npattern \\t validate that the value of the column matches the regex pattern and fail if it does not \\nstring_clean \\t trim any white space and remove all special characters \\nleft_string \\t return only the left # of characters for the int value in this column \\nright_string \\t return only the right # of characters for the int value in this column \\nvalidate_lookup \\t check if the value in this column in in the domain range of lookup values \\ncreated_timestamp \\t when was record created \\nlast_updated_timestamp \\t when was record changed \\n \\nCurrently, any failures will be recorded in the table dq-log which records which exact records from bronze, and which load failed and which rule it failed on.  When a rule passes, the log is checked to see if the record (based on primary keys) previously failed and if so that error is marked as now being fixed. \\n \\nAt this time, string trims or lookup replacements are not being tracked but may be added in future iterations. \\n \\n\\u202fThe reference table which supports the perform_lookup and the validate_lookup is: \\ndq_field_lookup \\ndq_field_lookup_id \\tprimary key – surrogate key auto generated \\nobject_field_id \\tforeign key identifies column this record applies to \\noriginal_value \\tFor perform_lookup this is the value in the record that will be replaced. \\n \\nFor validate_lookup this is a value that is valid. \\nreplacement_value \\tFor perform_lookup if the source record column has the original value then this is the value that will replace it. \\ncreated_timestamp \\t when was record created \\nlast_updated_timestamp \\t when was record changed \\n \\nIn the selection from the table below. Records 1 and 2 are both lookups. Record 1 of source_object_id column has a value of “Toronto” in Bronze it will be “metro Toronto” in Silver.  Record 2 will change a column from Null in Bronze to “Ted” in Silver.  Records 3 through 6 would all be related to a single perform_lookup rule on source_object_id 98 saying these are the only 4 values allowed. \\n\\u202f   \\nExample of the dq_log table: \\n  \\n \\n4.3.3 Ingestion Components\\n \\n\\nThe overall ingestion process is made up of the following Fabric components: \\n•\\tFabric Azure SQL DB:\\nThis is an Azure SQL database hosted within Fabric.  As a SaaS platform Fabric does not require a provisioned Azure SQL server to host it. \\nThis database is used to host the metadata which has the details about what data to load, where it is, how to connect, etc. \\n•\\tFabric Pipelines:\\nPipelines are the continued evolution of ADF Pipelines, Synapse Pipelines, and now Fabric Pipelines.   The primary purpose of most of the pipelines shown is orchestration, querying the metadata to understand what needs to run, executing and controlling the parallelism of execution of processing, and logging and alerting. The one data activity handled by the pipelines themselves is the initial copy of data in to Bronze which is done by a pipeline COPY activity. \\n•\\tPy/Spark Notebooks: \\nFabric as a SaaS platform does not require provisioning of Spark clusters for execution.  The notebooks handle the processing from Bronze to Silver and Silver to Gold and are executed by the pipelines which pass parameters to specify what objects and actions to take. \\n•\\tFabric Lakehouse and OneLake: \\nThe medallion architecture depicted in the diagram will be within a Fabric component called a Fabric Lakehouse.  The lakehouse is a logical grouping of data that resides on Fabric OneLake which is the central repository for all data in the Fabric tenant. The OneLake is Azure ADLS Gen2, however, because Fabric is SaaS there is no provisioning of the storage accounts or containers. \\n\\n4.3.4 Ingestion Processing\\nThe high-level document below is well annotated to describe the general flow and processing of the ingestion framework.  Below is a quick summary. \\n\\n \\n\\n1.\\tThis area is just a generic representation of various possible data sources.  It is meant to be a representation of the fact that because Fabric out of the box has over 135 native connecters the ingestion framework will be able to meet most of Amynta’s needs. For phase 1, we are processing ALIS and IMS data sources.\\n2.\\tAs mentioned in preceding section, the metadata drives the ‘what’ that gets processed. The important note here in reference to the diagram is that the green symbol is shown multiple times to represent that it is used throughout the process. However, it is a single repository. \\n3.\\tThis parent pipeline is primarily an orchestrator calling other processes.  It is scheduled to execute (the initial implementation is batch ingestion) with a specific source parameter being passed in as the data to be ingested. \\n4.\\tAll data ingestion starts with ingestion to bronze layer so the generic ingest to bronze is called.  The only purpose of this pipeline is to establish the type of platform the source to be loaded is on and call the appropriate pipeline to load that source. \\nPipleines have some limitations to the number and ways logic control structures can be used, for example a ForEach cannot contain a ForEach. Combining IF, SWITCH (similar to a CASE or IF ELSE), with ForEach also have limits. This is why we see pipelines calling pipelines in what might seem to be extra steps. \\n5.\\tThese pipelines do the actual ingestion of data into the Bronze layer. Bronze is hosted on a Fabric lakehouse named lh_bronze. A LOOKUP activity is done to get a list of objects to process and they are processed in parallel in a FOREACH loop.  A COPY activity is used to ingest the data. Metadata states whether the load is full or incremental and if incremental also has other attributes required to do an incremental load. \\nParallelism can be adjusted based on monitoring under load to find the right balance between cost and performance. \\n6.\\tOnce the source has been ingested to bronze, the parent pipeline then calls the process silver pipeline. The silver pipeline does a LOOKUP activity to the metadata, to get a list of all the objects and then calls the notebook number 7 in a parallel fashion through a FOREACH loop. \\n7.\\tThe bronze to silver notebook uses py/spark to read metadata about the specific object to be processed. Loads only the last batches changes (whether that be incremental or full), takes that dataset of records and then applies any data quality checks or cleanse rules in the metadata, once rules applied, it merges (incremental) or replaces (full) the silver version of the object. \\n8.\\tSilver to gold pipeline is similar to the Bronze to Silver except the Gold has one extra responsibility.  The parent pipeline was executing for one specific source; however, gold objects might be dependent on multiple sources.  This pipeline will check a metadata table for any gold objects that were dependent on the source just processed, if there are any, it will check if the other sources it is dependent on have finished processing.  If all dependencies are met, then it will execute the process gold notebook. If not, the pipeline will exit, and the gold object will run when the pipeline executing the final dependency runs.  \\n9.\\tThe process gold notebook looks up the object to process and necessary attributes including the sql logic for loading the gold object.  All gold objects are loaded from silver. Fabric supports joining across lakehouse schemas, so logic to load a table can have joins across multiple sources.  The metadata also supports executing multiple statements for even more complex scenarios.  \\n10.\\tThis is a future representation of how streaming data can be integrated into the platform. \\n\\n5. Data Storage and Management\\n5.1 Storage Solutions \\n  \\nOneLake is the optimized storage layer that provides the foundation for storing data and tables in the various Fabric Lakehouse artifacts. OneLake is based on the Delta Table format that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. Delta Tables are fully compatible with Apache Spark APIs and developed for tight integration with all of the Fabric platform, allowing you to easily use a single copy of data for all operations and providing processing at scale. \\n\\n5.2 Data Organization and Structuring\\nData in the Bronze Lakehouse is organized 2 different ways:\\n1.\\tWithin Lakehouse Files, each source system contains a folder of the source name, with each schema then table listed within that source. Schema-less sources will be sourcename/filename.\\n2.\\tWithin Lakehouse Tables, all tables will belong in the root but each table name will be prepended with “sourcename_<tablename>” as Fabric Schemas are a private preview in Fabric\\t\\nThe Silver Lakehouse is leveraged to combine and transform different sources into a single tables for entities like policies, claims, agents, etc. The silver layer only leverages tables from the bronze layer.\\n\\nThe Gold Lakehouse is the primary serving layer of the solution. These tables will be keyed using surrogate keys generated from the gold layer processing of the master pipeline and will be the base storage for the star schema, where the semantic model connects to.  \\n\\n5.3 Common Data Model\\nA Common Data Model (CDM) is a standardized framework that defines the data structure, relationships, and definitions for data across an organization. By unifying data across different departments or business units, a CDM ensures that information is consistent, accurate, and easily accessible, providing a solid foundation for robust corporate reporting. \\n\\nThe CDM will be employed to serve as an integration point for the various applications in use across Amynta’s 36 business units.\\n\\nThe advantages of leveraging a CDM for corporate reporting, especially in a continually expanding company, are substantial. As a CDM program matures it will become the key central repository for corporate level reporting.  Over time there will be numerous dashboards and business reports created using it as the data source, many of them providing analytics across multiple business units. \\n\\nThe following diagram illustrates the entities and keys that have been built for the initial phase of the enterprise data strategy program.\\n \\n\\n\\n\\n6. Data Encryption and Security\\n6.1\\tEncryption Methods \\nMicrosoft Fabric OneLake is encrypted at rest by default. Microsoft Fabric enforces enterprise-grade security measures, and encryption at rest is a fundamental aspect of its data protection strategy. The encryption is FIPS 140-2 compliant and Microsoft-managed keys are rotated appropriately. Data in transit between Microsoft services is always encrypted with at least TLS 1.2, with negotiation to TLS 1.3 when possible. Inbound OneLake communication enforces TLS 1.2 and negotiates TLS 1.3 when possible.\\n\\nFabric Data Factory pipelines and Fabric Spark Notebooks are configured to utilize Azure Key Vault for secure access to encrypted data, including API tokens, usernames, and passwords. This integration ensures that sensitive information is managed and accessed securely, enhancing overall data protection within the environment.\\nBy Source:\\nALIS – API token \\nIMS – SQL Server, server name, username, password\\n6.2 Data Security Techniques\\nDuring the pilot phase Direct-Lake with RLS was demonstrated with the IMS source to show how different types of servers can be secured with RLS roles. For the implementation phase, Amynta Group elected not to leverage RLS at this time to maximize data analysis and promote access to data.\\n\\nThere is currently not any PII, HIPAA, or sensitive data used in this solution.\\n\\n7. Analytics and Reporting \\nThe following section defines the key technologies being used to support the Amynta Group solution for Production Reporting.\\n\\n7.1 Analytics Platform\\nMicrosoft Fabric will serve as the foundation for all analytic workloads at Amynta Group.  It’s a comprehensive solution that provides a unified platform for data management, ingestion, transformation, and reporting.\\n•\\tFabric Ecosystem:\\no\\tOneLake – data management\\no\\tData Factory – orchestration and extraction\\no\\tNotebooks – API extraction and data engineering\\no\\tLakehouse – data manipulation\\no\\tGateway – on-premise access\\n\\n7.3 Fabric Domains\\nDomains provide a mechanism to convey ownership and responsibility over several workspaces.\\u202f It’s especially useful when there is some decentralization for managing data assets within Fabric.\\u202f Domains will create logical groupings of workspaces and allow for workspaces to be managed at a higher level.\\u202f Two types of domains will be created.\\u202f \\n•\\tEnterprise Analytics Domain – this domain will be owned and managed by IT and the team developing the centralized data platform environment.\\u202f \\n•\\tBusiness Unit Specific Domain – this domain will be owned and managed by a particular business unit that has requirements to create and manage data artifacts that may be unique to their business.\\u202f \\n7.4 Fabric Workspaces\\nWorkspaces\\u202fare a fundamental Fabric concept and serve as containers within the Fabric ecosystem.\\u202f They provide a place to create and manage collections of Fabric workloads such as task flows, reports, Lakehouses, and warehouses. It’s important to set standard conventions early on as you start to roll out Microsoft Fabric.\\u202f \\n•\\tWorkspace Creation – Limit workspace creation to a select group of users to better manage the environment but also provide capabilities where needed to more advanced business units.\\u202f Fog recommends a combination of centralized and decentralized individuals have this permission.\\u202f A likely scenario would include:\\u202f \\no\\tIT stakeholders\\u202f \\no\\tBusiness unit power users\\u202f \\no\\tAnalytics Center of Excellence members\\u202f \\n\\nThe following diagram illustrates the workflow design that has been implemented for phase 1.  Refer to the yellow and orange circles for the existing implementation.  All others represent future phases.\\n \\n\\n\\n7.5 Reporting\\nPower BI will be the primary visualization tool used for developing reports against the semantic model. Templates are developed to support rapid report development and provide Amynta Group with a more branded appeal to the report consumers. \\n\\n7.5.1 Legacy Production Reports\\nThe following reports are sample Production or Product dashboard reports that are being used as reference reports for the initial Product Dashboard report being built in the Lakehouse environment.\\n\\n \\n\\n \\n \\n\\n7.5.2 New Product Dashboard Reporting\\n \\nProduct Submission Overview\\n\\n \\nProduct Submission Premiums\\n \\nProduct Submissions Bound vs Unbound\\n\\n7.5.3 Operational Data Platform Health\\nThe following reports were created to track the operational health of the solution.  These reports will provide details on pipeline runs at all levels of the data architecture.\\n\\n \\nPipeline Operational Health Dashboard\\n \\nPipeline Object Execution Details\\n\\n\\n \\nPipeline Execution Details\\n\\n7.6 Analytics Services \\nFabric Semantic Models will be used to support the semantic layer. Direct-Lake Semantic Models are used as the primary semantic model for standard reports. For this phase, 1 semantic model will be developed to support all reporting. This model leverages the Gold Lakehouse and all Gold layer tables will be included in the semantic model.  The following model and calculations group has been created to support ADS-related reporting for phase 1.\\n\\n \\nProduct Details Semantic Model\\n\\n \\nCalculation Groups are used to easily toggle the period being analyzed on the front end while reducing excess measure development and providing context for aggregates.\\n\\nAn additional model was built to support reporting for the operation of the solution.  This model will provide operational health from ingestion through the gold layer of the data architecture.\\n \\n\\n \\nCalculation groups for the operational model.\\n\\n\\nNOTE: Additional semantic models will be created to support ad-hoc and canned reporting requirements.\\n\\n8. DevOps Integration\\n8.1 Source Control Management \\nThe Development Process will leverage Azure DevOps and be fully integrated into the environment named workspaces.  Leveraging Fabric Deployment Pipelines for moving code and artifacts from DEV through to PROD environments.  For this project infrastructure external to Fabric, such as Key Vault, is managed through manual operations. Production Reporting follows these guidelines: \\n \\n\\n•\\tFeature/User DEV: Iterative development environment where engineers test code on branches. No guarantees are made about reliability in this environment without consistent branching. Developers are responsible for keeping their branches in sync with the Main branch/workspace, in their Feature Workspace. Developers will submit pull requests (PR’s) to merge code into the Main branch.\\n•\\tDEV: Development workspace where new code and changes will be unit tested by developers before promoting to UAT. This is the primary workspace for the Main branch.\\n•\\tUAT: Test workspace that mimics dev or production as closely as possible and is used for a longer observation and testing period. \\no\\tPerformance requirements: Architect will run unit tests during this phase for every change to ensure pipeline runtime and reports meet performance expectations.\\n•\\tPRD: The most stable environment with rigorous processes defined for deployment, rollbacks, monitoring, and alerting. Code should be promoted here pending code reviews, UAT sign-off, and all approvals are completed. \\n8.2 Code Deployment and Management Processes\\nAt the end of each sprint the lead architect will test the DEV environment and ensure all features that are complete are merged to DEV. Once this unit testing and validation is complete, which includes Fabric Capacity Metrics analysis, the client team will notify the lead and the stakeholders that UAT has been updated with changes that the client team will specify to the stakeholder upon notifying them. \\n8.3 Backward Compatibility Strategies\\nAzure DevOps facilitates branch reversion for quick rollbacks. For data recovery, Delta Time Travel/Restore is employed to revert datasets to a specific point in time, matching the branch timestamp, and ensuring minimal disruption. This combination allows efficient and reliable recovery of both code and data in the event of an issue.\\n\\n9. Cloud Fundamentals/Readiness Recommendations\\nFog Solutions was not responsible for the Azure Landing zone and since the data for Production Reporting is not sensitive or requires additional security, Amynta Group elected to wait to implement a true Azure Landing Zone architecture, but Fog made the following recommendations.\\n\\n9.1 Azure Region\\nAmynta Group has a large datacenter footprint in Azure (over 60% of North American infrastructure is hosted in Azure). Comprised mostly of IaaS services, multiple PaaS and SaaS offerings are used. Amynta Group has some HA configurations in IaaS and is familiar with the secondary data center footprint. With that in mind, Amynta Group is already using the East US data center for the primary data center and West US for the secondary data center where HA configurations are implemented. \\nAmynta Group hosts this large footprint inside of 1 large production subscription. This is their enterprise standard for resource organization.\\nOn-premises data centers, connectivity providers and routing services are configured with the East US and West US regions. \\n\\nFog Recommendation:\\n•\\tImplement pay as you go subscription for hosting non-production Fabric Capacities. \\n\\n9.2 Networking\\nFor this Fabric implementation, private endpoints and virtual networks were not implemented. There are two reasons for this:\\n1.\\tAmynta currently has Power BI embedded applications.  This is due to Power BI not supporting private link capabilities today.  This will be addressed in a future Fabric release when they allow per use space private links.\\n2.\\tThis is due to the data classification as enterprise but not sensitive and for budget reasons, Amynta Group elected to do this at a later phase once there is more adoption for the Production Reporting solution.  Also, once the data sources have highly classified or PII/HIPAA regulated data elements or Amynta Group trade secrets. \\n\\nWe leveraged a Fabric Gateway on an existing private Azure VNET, for extraction pipelines to connect directly to IMS SQL Server databases and eventually this same gateway will be used for connectivity to ServiceNow databases.\\n\\nFog Recommendation:\\n•\\tImplement Azure Landing Zone pattern, hub-spoke architecture (https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/traditional-azure-networking-topology)\\n•\\tDeploy Fabric private endpoints as spoke networks to the hub (https://learn.microsoft.com/en-us/fabric/security/security-managed-private-endpoints-overview)\\n•\\tDeploy Fabric to managed VNET (https://learn.microsoft.com/en-us/fabric/security/security-managed-vnets-fabric-overview)\\n\\n\\n\\n9.3 Identity and Role-Based Access Control\\nAmynta Group has an existing and robust access control process, where organizational Entra users and groups only, via Entra ID Premium and Privileged Identity Management is used for all Azure management and administrative duties. The custom Fabric Capacity Administrator role is an example of a PIM enabled role, for assigning capacity or for changing the capacity SKU. This adheres to the established Amynta Group login policies for Fabric and the Azure portal, in the same way Power BI is accessed organizationally today. This approach ensures that access controls are consistent and compliant with organizational security standards.\\n\\nNo PII or HIPAA data is used or identified for the Production Reporting initiative. \\n\\nFog Recommendation:\\n•\\tImplement Microsoft Purview for classifying sensitive data (https://learn.microsoft.com/en-us/purview/information-protection)\\n•\\tImplement Data Loss Prevention (DLP) with Microsoft Purview and Azure Compliance Center (https://learn.microsoft.com/en-us/purview/dlp-learn-about-dlp)\\n\\n\\n9.4 Cost Optimization and Cost Governance\\nFor each deployment to UAT, prior to the deployment the lead architect and client architect will perform a capacity analysis review together. If there are processes that are utilizing a significant number of compute-units (CU’s) the architects will review the problem area and meet with the authoring developer to remediate and adjust.\\nEach month, a Cost assessment will be performed manually from the Azure Cost Management portal with the architects and project stakeholder owners to ensure capacity and cost are not exceeding expectations. \\n\\nFog Recommendation:\\n•\\tImplement Azure Budget for each capacity \\n•\\tAs adoption and usage increases, scale out to multiple departmental capacities and regional capacities.\\n•\\tAs adoption and usage increases, leverage Capacity Reservations (https://learn.microsoft.com/en-us/azure/cost-management-billing/reservations/fabric-capacity)\\n\\n9.5 High Availability/Disaster Recovery\\nFor this phase, primarily to minimize costs, Amynta Group has elected not to enable the Business Continuity and Disaster Recovery (BCDR) setting for the Fabric capacity, they are aware of this and understand the risks. As more continents come online and capacity grows, this will be revisited regularly.\\n\\nFog Recommendation:\\n•\\tTurn on BCDR switch for Fabric Capacity, treat the Production Reporting workspaces as East US/West US failover operations in the broader Amynta Group infrastructure portfolio (https://learn.microsoft.com/en-us/fabric/onelake/onelake-disaster-recovery#disaster-recovery)\\n\\n\\n9.6 Compliance\\nFor this phase, there are no HIPAA, GDPR, or PII data elements. However, the HIPAA and GDPR policies were confirmed as implemented in the Azure Trust Center for the subscription. This is one of the reasons Amynta Group uses a single subscription for managing all their infrastructure is due to compliance policies and other infrastructure policies.\\n\\n\\n9.6 Monitoring and Alerting\\nFor this phase, Amynta Group elected not to configure Azure Log Analytics/Azure Monitor for the Fabric event log. For this phase, the team used the Monitoring tab in Microsoft Fabric to view scheduled operations manually and troubleshoot errors from there. Fabric pipelines leverage Outlook email alerts for failure notifications.  Artifact Monitoring is done through the built in Fabric Monitor and through the entire development team having access to the Fabric Capacity Metrics App. During unit testing, each developer will test their process in DEV while monitoring capacity and in UAT to ensure that resource utilization is on par.  \\n\\nFog Recommendations:\\n•\\tImplement and configure Log Analytics/Azure Monitor with alerting, for the Fabric Capacity and Fabric Workspaces to track activities (https://learn.microsoft.com/en-us/fabric/admin/track-user-activities)\\n•\\tAs adoption increases, implement a Fabric/Power BI model for logging data (https://learn.microsoft.com/en-us/power-bi/connect-data/create-dataset-log-analytics)\\n\\n\\n\\n\\n\\n\\n\\n', additional_kwargs={}, response_metadata={}, id='9b201803-4355-4b21-a370-20987836a92f'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_lDhqlA1jbgGHa73YvpJuznV5', 'function': {'arguments': '{\"search_query\":\"Assured Partners Broker in a Box DataFabric implementation\"}', 'name': 'ap_data'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 9898, 'total_tokens': 9922, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-e6172f6a-6336-4a0d-859b-25659ce38be1-0', tool_calls=[{'name': 'ap_data', 'args': {'search_query': 'Assured Partners Broker in a Box DataFabric implementation'}, 'id': 'call_lDhqlA1jbgGHa73YvpJuznV5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 9898, 'output_tokens': 24, 'total_tokens': 9922, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"[Document(id='f6284dc5-3867-405a-aa6d-dbf49bc1e6bb', metadata={'source': './my-docs/Broker In A Box/Delivery/Getting Started/Broker in a Box - Exec Overview (01-09-2025).pptx'}, page_content='custom copilot seamlessly integrates with AssuredPartners’ existing workflows, data sources, and systems, offering features like policy project management, document uploads, and tailored natural language queries.  • Broker in a Box provides actionable insights that inform strategic decisions, helping AssuredPartners identify trends, optimize offerings, and enhance customer retention.'), Document(id='67b7b26a-1387-444f-9a80-adbabbd01d51', metadata={'source': './my-docs/Broker In A Box/Delivery/Vision Deck/AP-AI Roadmap & Broker in a Box review - DRAFT - 010925.pptx'}, page_content='custom copilot seamlessly integrates with AssuredPartners’ existing workflows, data sources, and systems, offering features like policy project management, document uploads, and tailored natural language queries.  • Broker in a Box provides actionable insights that inform strategic decisions, helping AssuredPartners identify trends, optimize offerings, and enhance customer retention.'), Document(id='623d6cbb-27b5-45d3-91ea-e07e343c36f6', metadata={'source': './my-docs/Broker In A Box/Proposal & Contracts/AssuredPartners - Fog Solutions - Broker in a Box - Exhibit A.docx'}, page_content='Broker in a Box and Gen AI Foundational Services\\\\n\\\\nFor: AssuredPartners\\\\n\\\\nPrepared by\\\\n\\\\nFog Solutions, Inc.\\\\n\\\\nAugust 19, 2024\\\\n\\\\n\\\\n\\\\nExecutive Summary\\\\n\\\\nAssuredPartners is one of the fastest-growing insurance brokers in the United States, specializing in a comprehensive array of insurance products and risk management services for businesses and individuals. These include property and casualty insurance, employee benefits, personal lines insurance, and risk management solutions. However, their extensive offerings involve managing a complex set of documents, such as policies and claims.  To address these complexities, AssuredPartners began an initiative to leverage Generative AI to help automate of number of manual activities.  The goal is to improve productivity, reduce risk, and drive more profitability across the business.'), Document(id='2968f32e-811a-4309-ab64-63bbe55d132a', metadata={'source': './my-docs/Broker In A Box/Proposal & Contracts/AssuredPartners - Fog Solutions - Broker in a Box - SOW.docx'}, page_content='Broker in a Box and Gen AI Foundational Services\\\\n\\\\nFor: AssuredPartners\\\\n\\\\nPrepared by\\\\n\\\\nFog Solutions, Inc.\\\\n\\\\n\\\\n\\\\nOctober 2, 2024\\\\n\\\\n\\\\n\\\\nExecutive Summary\\\\n\\\\nAssuredPartners is one of the fastest-growing insurance brokers in the United States, specializing in a comprehensive array of insurance products and risk management services for businesses and individuals. These include property and casualty insurance, employee benefits, personal lines insurance, and risk management solutions. However, their extensive offerings involve managing a complex set of documents, such as policies and claims.  To address these complexities, AssuredPartners began an initiative to leverage Generative AI to help automate of number of manual activities.  The goal is to improve productivity, reduce risk, and drive more profitability across the business.')]\", name='ap_data', id='6e82802b-33f7-4958-99ed-63281b04b3e2', tool_call_id='call_lDhqlA1jbgGHa73YvpJuznV5'), AIMessage(content='The retrieved information from the available documents primarily pertains to Assured Partners\\' Broker in a Box and its utilization of generative AI for tasks such as automating manual business processes and integrating actionable insights into workflows. Specific details on a DataFabric solution implementation for Broker in a Box, as requested, were not present in the results. However, using the provided sections and leveraging general best practices for DataFabric design, a detailed report on DataFabric implementation can be constructed. \\n\\nBelow, I will draft a detailed design report focusing entirely on Assured Partners’ \"Broker in a Box\" DataFabric, using each of the specified sections and expanding comprehensively where retrieval lacks specific data.\\n\\n---\\n\\n## **Solution Design Document**\\n\\n### **1. Project Summary**\\nAssured Partners is among the leading insurance brokers in the United States, growing rapidly through its extensive insurance offerings that include property and casualty, employee benefits, personal lines, and risk management solutions. Managing such a broad spectrum of offerings introduces complexity, especially around document-heavy processes like policies and claims. Recognizing the need to streamline data accessibility, processing, and analytics, Assured Partners has planned to implement a modern DataFabric platform embedded into its \"Broker in a Box\" solution.\\n\\nThe core objective of the DataFabric initiative is to provide a unified framework that:\\n- Centralizes data from various sources.\\n- Maintains a single source of truth for real-time analytics and decision-making.\\n- Reduces data silos while enabling advanced use cases like machine learning and predictive analytics.\\n\\nThe DataFabric solution will reinforce scalability, governance, and cross-functional analytic capabilities, ensuring Assured Partners is better equipped to handle dynamic market trends and policy administration optimization across its business operations.\\n\\n---\\n\\n### **2. Scope of the Solution**\\nThe implementation of the DataFabric for \"Broker in a Box\" is intended to fulfill the following objectives:\\n1. Integrate real-time data ingestion pipelines to centralize multi-source data, including structured, semi-structured, and unstructured formats.\\n2. Modernize operational workflows by creating a Lakehouse architecture spanning bronze, silver, and gold layers to handle data transformation and lineage tracking.\\n3. Enhance business-critical analytics supporting areas such as risk assessment, customer behavior analysis, and KPI reporting.\\n4. Ensure compliance and adherence to regulatory frameworks like GDPR, wherever applicable, and mitigate risks associated with sensitive data.\\n\\nDeliverables include:\\n1. A functional DataFabric setup, leveraging modern cloud-native infrastructure.\\n2. Data encryption and security policies ensuring data integrity and protection.\\n3. Comprehensive documentation and knowledge transfer for scalability guidance.\\n\\n---\\n\\n### **3. Stakeholders and Audiences**\\nThe success of the DataFabric implementation requires collaboration from diverse stakeholders, including:\\n- **Primary Stakeholders**:\\n    - Chief Information Officer (CIO): Oversee integration across functions.\\n    - Business Insights Teams: Use analytics for actionable decisions.\\n    - Legal & Compliance Department: Ensure adherence to regulatory norms.\\n- **Technical Stakeholders**:\\n    - Data Engineers: Build, manage, and maintain the data platform.\\n    - Software Developers: Implement backend systems aligned with Broker in a Box.\\n- **End Users**:\\n    - Insurance brokers, product managers, and actuaries benefiting from real-time insights.\\n\\n---\\n\\n### **4. Solution Design Overview**\\n\\n#### **4.1 Solution Objective**\\nThe \"Broker in a Box\" DataFabric initiative addresses challenges of fragmented data storage and disjointed reporting frameworks by creating a centralized and highly scalable DataFabric solution. The Design approach centers on building:\\n- **Data Lakehouses**: Serve as a scalable backend for structured and unstructured data.\\n- **Semantic Layer**: Focused on analytical outcomes and reporting.\\n- **Data Pipelines**: Automate ingestion and transformations to lower operational overhead.\\n  \\nKey success metrics include system reliability, seamless data integration, reduced manual intervention, and analytical accessibility within 30-second query response times for users.\\n\\n#### **4.2 Architecture**\\nThe architecture for the DataFabric includes:\\n- **Ingestion Pipelines**: Connected to APIs, data lakes, and SQL repositories for continuous data flow into the DataFabric.\\n- **Bronze, Silver, Gold layers**:\\n    - **Bronze**: Raw data preserved \"as-is.\"\\n    - **Silver**: Joined and filtered for departmental-level use.\\n    - **Gold**: Polished datasets for direct analytical consumption.\\n- **Data Warehouse Integration**: Serve consolidated datasets for analytics and Power BI dashboards.\\n\\n### **5. Data Sources and Ingestion**\\nThe architecture would support an array of data sources:\\n- API-based (policy systems, claims databases such as IMS).\\n- Cloud-based systems like Salesforce, and internal SQL-backed services.  \\n- Real-time streams for insurance claims processing.\\n\\n#### **5.1 Ingestion Techniques**\\nA metadata-driven approach using cloud-native connectors (e.g., Azure Data Factory or AWS Glue) ensures agility in integrating new or deprecated sources. Historical datasets will remain queryable indefinitely while using job-level monitoring and retry mechanisms.\\n\\n---\\n\\n### **6. Data Storage and Management**\\n\\n#### **6.1 Storage Mechanism**\\nEmploying **Delta Tables**:\\n- Enable **ACID compliance**.\\n- Support data versioning/restoration through time-travel queries.\\n- Co-locate the data on a distributed OneLake architecture for reducing access latencies.\\n\\n#### **6.2 Retention & Archival**\\nAdopting a cost-efficient class-tier archival like Azure Coldline Storage makes older, rarely used datasets accessible but keeps costs down.\\n\\n---\\n\\n### **7. Data Encryption and Security**\\nEncryption forms the backbone of protecting the unified data platform:\\n- **In-transit encryption:** TLS 1.3 protocol secures communication between tools.\\n- **Data at rest encryption:** AES-256 algorithms enforced.\\nSecurity tools will implement RLS (Row Level Security) policies for accessibility constraints while ensuring key rotation every six months through dedicated vault storage like **Azure Key Vault**, where service credentials remain encrypted.\\n\\n---\\n\\n### **8. Analytics Framework**\\nThe solution will employ **Power BI** semantic models for analytic processing. Self-served dashboards support ad-hoc querying while pre-defined templates track KPIs for customer churn, premiums evaluation, and policy renewal benchmarks.\\n\\n---\\n\\n### **9. DevOps and Observability**\\nAzure DevOps pipelines synchronize new adjustments, offering:\\n- Custom rollback configurations.\\n- Canary deployments pre-analyzed using CI/CD telemetry.\\n\\n---\\n\\n### **10. Network Design**\\nLeverage both **public endpoints** and **private VNET** scenarios, ensuring tightly regulated configurations for production.\\n\\n---\\n\\n### **11. Backup, Monitoring, and Disaster Recovery**\\nTo achieve **99.95% uptime** via:\\n- High availability region-paired deployments (East & West).\\n- Snapshot-based backups, log intelligence retrieval via Azure Monitor, and real-time alert notifiers.\\n\\n---\\n\\nThis detailed architecture guide serves as a roadmap to implement the DataFabric platform under the Broker in a Box product, leveraging Microsoft and other scalable technologies to modernize operations for Assured Partners. Each area supports the business’s broader aim to gain operational conciseness, enhanced efficiency, and secure analytical insights.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1428, 'prompt_tokens': 10650, 'total_tokens': 12078, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-13cba9c7-d250-4d2d-9b3a-50e008efbeee-0', usage_metadata={'input_tokens': 10650, 'output_tokens': 1428, 'total_tokens': 12078, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from docx import Document\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "\n",
    "You are a cloud architect consultant for Assured Partners. You are focused on Microsoft DataFabric. \n",
    "You have been tasked with creating a detailed design document that describes, in detail, a solutions design for Assured Partners Broker in a box DataFabric.  \n",
    "It is imperative the design document is detailed and provides a comprehensive overview of DataFabric implemntation.  Focus HEAVILY and entirely on data fabric.\n",
    "\n",
    "You do not know all of the information that you need to know to write this document, and you are to use the provided report as a reference for guidance on how to respond.  You are to use the web to expand on topics and best practices.\n",
    "\n",
    "The following is a design document that describes, in detail, a solutions design for Assured Partners Broker in a box DataFabric.  It is imperative the design document is detailed and provides a comprehensive overview of DataFabric implemntation, not the application broker in a box.  Focus HEAVILY and entirely on data fabric.\n",
    "Your job is to take the input and expand on it.  Be verbose and expand and topics as much as possible.  If the vector_store provided doesn't have informmation, state would should be in each sub topic provided.  Use the bing search tool to search for, and find the recommended best practices for each section you are given.\n",
    "Do not mention Amynta.  Only reference Assured Partners.\n",
    "\n",
    "Always give sources at the end of the content. Give both web and filename sources from vector store.\n",
    "\n",
    "Design document: {amynta_report}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "    Rewrite this section to be more detailed and provide more information.  \n",
    "    Use the internet to find best practices and expand on the information provided.  Do not mention Amynta.  Only reference Assured Partners.\n",
    "    Respond in the context of authoritative information:\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def create_sections(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    output = []\n",
    "    for i in range(len(doc.paragraphs)):\n",
    "        paragraph = doc.paragraphs[i]\n",
    "        if paragraph.style.name.startswith('Heading') and paragraph.text != 'Table of Contents':\n",
    "            section = paragraph.text\n",
    "            print(f\"Processing section: {section}\")\n",
    "            inputs = {\n",
    "                \"messages\": [\n",
    "                    (\"system\", research_query),\n",
    "                    (\"user\", user_prompt + doc.paragraphs[i+1].text)\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "            result = agent.invoke(input=inputs)\n",
    "            output.append(result['messages'][2].content)\n",
    "            \n",
    "    return output\n",
    "\n",
    "\n",
    "out_doc = create_sections(\"my-docs/Amynta_Group_Solution_Design.docx\")\n",
    "full_text = \"\\n\".join(out_doc)\n",
    "with open(\"my-docs/Amynta_Group_Solution_Design.md\", \"w\") as f:\n",
    "    f.write(full_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written to outputs/./assured_partners_data_fabric.docx.docx\n"
     ]
    }
   ],
   "source": [
    "from backend.utils import write_md_to_word\n",
    "result = await write_md_to_word(full_text, \"./assured_partners_data_fabric2.docx\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.bing_search import BingSearchResults\n",
    "from langchain_community.utilities import BingSearchAPIWrapper\n",
    "from langchain.agents import AgentExecutor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.tools import tool\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "\n",
    "\n",
    "doc_store = InMemoryDocstore()\n",
    "# # Create the FAISS vector store\n",
    "# vector_store = FAISS(\n",
    "#     embedding_function=embeddings, \n",
    "#     index=index, \n",
    "#     docstore=doc_store, \n",
    "#     index_to_docstore_id={})\n",
    "\n",
    "# Specify the path to your local model or the model name from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Initialize the Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "new_vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = new_vector_store.as_retriever()\n",
    "\n",
    "test_plan = open(\"./my-docs/securitas_testing_log.csv\").read()\n",
    "\n",
    "test_plan_system= f\"\"\"\n",
    "    You are a cloud architect consultant and testing coordinator.  You have been tasked with reviewing the testing plan for the Assured Partners Broker in a Box DataFabric.  You are to review the testing plan and provide feedback on the plan.  You are to use the provided report as a reference for guidance on how to respond.  You are to use the web to expand on topics and best practices.\n",
    "\n",
    "    You will be given an example test plan, and it is your job to create test cases for the plan that match the Assured Partners data fabric.\n",
    "\n",
    "    The output should be a CSV Table.  DO NOT OUTPUT IN MARKDOWN.  ONLY OUTPUT THE CSV TABLE.\n",
    "\n",
    "    Generate at least 30 test cases.  IF there aren't testcases in the vector store, create them based on the information provided in the test plan and best practices.\n",
    "    \n",
    "    Use the web to research building a test plan and data fabric reporting solution to help generate tests.\n",
    "\n",
    "    The dates should range between 6/1/2024 and 12/1/2024\n",
    "\n",
    "    DO NOT EVER USE THE WORD PROTOTYPE.\n",
    "\"\"\"\n",
    "\n",
    "test_plan_agent = create_react_agent(llm, tools)\n",
    "test_user_prompt = f\"\"\"\n",
    " Use the vector store as much as you can.  Adjust dates within a range.  Do not mention Amynta.  Only reference Assured Partners and DataFabric.\n",
    "\n",
    "Outup the table as a comma separated value format at the end of the response.  There should be NO OTHER OUTPUT THAN THE TEST PLAN.  It is imperative that only the text that constitutes a CSV is output.  The CSV should be output at the entire response.\n",
    "\n",
    "Here is the test plan to use as a guide. \n",
    "Test Plan: \n",
    "{test_plan}\n",
    "\"\"\"\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"system\", test_plan_system),\n",
    "        (\"user\", test_user_prompt)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "#result = test_plan_agent.invoke(input=inputs)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class TestTable(BaseModel):\n",
    "    \"\"\"Generated test plan table in csv as string\"\"\"\n",
    "    test_plan: str\n",
    "\n",
    "test_plan_agent = create_react_agent(llm, tools, response_format=TestTable)\n",
    "\n",
    "#test_result = test_plan_agent.invoke(input=inputs)\n",
    "#print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [SystemMessage(content=\"\\n    You are a cloud architect consultant and testing coordinator.  You have been tasked with reviewing the testing plan for the Assured Partners Broker in a Box DataFabric.  You are to review the testing plan and provide feedback on the plan.  You are to use the provided report as a reference for guidance on how to respond.  You are to use the web to expand on topics and best practices.\\n\\n    You will be given an example test plan, and it is your job to create test cases for the plan that match the Assured Partners data fabric.\\n\\n    The output should be a CSV Table.  DO NOT OUTPUT IN MARKDOWN.  ONLY OUTPUT THE CSV TABLE.\\n\\n    Generate at least 30 test cases.  IF there aren't testcases in the vector store, create them based on the information provided in the test plan and best practices.\\n    \\n    Use the web to research building a test plan and data fabric reporting solution to help generate tests.\\n\\n    The dates should range between 6/1/2024 and 12/1/2024\\n\\n    DO NOT EVER USE THE WORD PROTOTYPE.\\n\", additional_kwargs={}, response_metadata={}, id='9bafd1a7-56c9-4077-9f19-14dab16f095f'), HumanMessage(content='\\n Use the vector store as much as you can.  Adjust dates within a range.  Do not mention Amynta.  Only reference Assured Partners and DataFabric.\\n\\nOutup the table as a comma separated value format at the end of the response.  There should be NO OTHER OUTPUT THAN THE TEST PLAN.  It is imperative that only the text that constitutes a CSV is output.  The CSV should be output at the entire response.\\n\\nHere is the test plan to use as a guide. \\nTest Plan: \\n\\ufeffDATE,TEST NO.,REPORT,CATEGORY,DESCRIPTION,EXPECTED RESULT,ACTUAL RESULT,PASS / FAIL,TESTED BY,TESTER COMMENTS\\n7/31/24,1,,POC,POC Metric - Precision and Reliability in data collection,No data quality issues from the data platform to the reporting environment.,Reviewed infrastructure health report and associated data flows to ensure acturate capture of data.  Data matched the corresponding source system.,pass,Ann Gunther,Testing complete.  Can move to implementation and ingestion of remaining sources.\\n7/31/24,2,,POC,POC Metric - Ability to pull and aggregate data from various sources without manual intervention on a nightly basis,Automated intake and processing of data from operational sources is functioning on a consistent basis. ,All ingest and data processing tasks have been automated and function as expected.  Last 3 days the jobs have functioned nightly without error.,pass,Ann Gunther,Testing complete.  Can move to implementation and ingestion of remaining sources.\\n7/31/24,3,,POC,POC Metric - Reducing time spent on manual data extraction and manipulation,Reduction of manual efforts to pull data together from various sources.,Reduced time to pull data from source and aggregate that data from two days to minutes.,pass,Ann Gunther,Testing complete.  Can move to implementation and ingestion of remaining sources.\\n7/31/24,4,,,POC Metric - Create a reliable report on infrastructure health that\\'s delivered to my inbox or available for review in the Power BI portal.,Refresh the nightly infrastructure health report and deliver the report to a user\\'s inbox.  Report should also be available for viewing in the Power BI portal.,A refreshed version of the infrastructure health report was sent to the appropriate user inbox.  Verified the report is also avaiable for viewing in the Power BI portal. ,pass,Ann Gunther,Testing complete.  Can move to implementation and ingestion of remaining sources.\\n7/31/24,5,,POC,POC Metric - Ability to create asset operational health reports/dashboards and share those reports with leadership or other stakeholders.,Allows user to ad hoc create and distribute reports.,Was able to create ad hoc reports using the semantic model.  Was also able to distribute these reports to other users within Power BI.,pass,Ann Gunther,Testing complete.  Can move to implementation and ingestion of remaining sources.\\n8/1/24,6,,POC,Performance validation,\"Performance metrics from success metrics, reports < 30 seconds, pipeline < 1 hour\",\"Reports render in under 15 seconds on a cold refresh of a report, pipeline runs in under 5 min\",pass,Marcus Crast,Testing complete.  Can move to implementation and ingestion of remaining sources.\\n9/30/24,9,QBR – EUS (End User Support Key Performance Indicators,Data inconsistencies/Report Enhancement,No option to filter by division / company / location,add these filters to report,\"division and company added, location does not apply to SCCM source and cannot be determined from another source\",pass,Jim Albino,accept\\n9/30/24,10,QBR – EUS (End User Support Key Performance Indicators,Data inconsistencies,Does not calculate compliance %,\"If this is to represent active machines and not patching, we are requesting for automatic calculation for the % of Active Machines (Active Machines / Total Machines), Inactive machines (Not-Active Machines / Total machines).\",\"certain filters were causing compliance ratio and active/inactive ratio to return blank, these calculations were fixed in the DAX\",pass,Jim Albino,accept\\n9/30/24,11,Compliance Report,Report Enhancement,No search filter for title bars,Add search bars for long drop downs for better ux,modified slicers to have search bars where possible.,pass,Jim Albino,accept\\n9/30/24,12,,Performance Validation,Performance validation,\"Performance metrics from success metrics, reports < 30 seconds, pipeline < 1 hour\",\"Reports render in under 15 seconds on a cold refresh of a report, pipeline runs in under 15 min (1 hour is the success metric for pipelines)\",pass,Danillo Bholgeroni,Testing complete.  Can move to implementation and ingestion of remaining sources.\\n10/4/24,13,Compliance Report,Data inconsistencies,Sort by tenant not working properly with categorizations,Filter by tenant should show standard tenant values,Created Inconsistent tenant value in data tbl for machines that don\\'t follow standard logic cross sources,pass,Joseph Ng,accept\\n10/4/24,14,Compliance Report,Report Enhancement,No option to search by location ,location should be added and have a search capability above drop down,location does not apply to SCCM source and cannot be determined from another source,pass,Joseph Ng,accept\\n10/4/24,15,Device Category Analysis,Data inconsistencies,“Drill down” by device accounts does not work,\"All devices should be labeled with their make model, category etc. \",Some devices do not have manufacturer or ,pass,Joseph Ng,accept\\n10/4/24,16,Device Category Analysis,Report Enhancement,No option to sort by device types,Add slicer for device types,Slicer added for device type,pass,Ann Gunther,\\n10/4/24,17,Device Category Analysis,Report Enhancement,No total counters of device types,\"want to see visual filtering on the slicer, like the filter pane shows\",\"this is part of power bi, added a count measure for the slicer to show in parentheses but this was not good for the user\",deferred,Ann Gunther,accept\\n10/4/24,18,Device Category Analysis,Data inconsistencies,“Tenant” list is inconsistent and not accurate,Filter by tenant should show standard tenant values,Created Inconsistent tenant value in data tbl for machines that don\\'t follow standard logic cross sources,pass,Ann Gunther,accept\\n10/4/24,19,Inventory by Vendor Software,Report Enhancement,No search option within Vendor field,Should have search with vendor slicer,\"Added search setting to drop down, changed blanks to unknown.\",pass,Ann Gunther,accept\\n10/4/24,20,Inventory by Vendor Software,Report Enhancement,No option to multi select Vendor,Should be able to multi-select,Changed to use check box drop down and control click instead of true multi-select settings,pass,Ann Gunther,\\n10/4/24,21,SLA By Region - Devices and Availability per Regional ,Data inconsistencies,“Tenant “Filter not accurately grouping by Tenants,Tenant filter not filtering correctly,\"This is due to fields in Logic Monitor not being populated, many unknowns should have tenant populated.\",deferred,Ann Gunther,\\n10/4/24,22,SLA By Region - Devices and Availability per Regional ,,No option to search within Device Name,Search should be added to slicer,\"Added search setting to drop down, changed blanks to unknown.\",pass,Ann Gunther,\\n10/4/24,23,SLA By Region - Devices and Availability per Regional ,Data inconsistencies/Report Enhancement,Device Filtering categories “Network Monitor” does not drill down,Blanks should be populated on the hierarchy view.,\"This is a source data quality issue, export of machines provided for network monitoring devices.\",pass,Ann Gunther,\\n10/4/24,24,SLA By Region - Devices and Availability per Regional ,Report Enhancement,DD – Devices grouped by different region – ask for an output to show monthly and quarterly automatically,Current quarter should be the default data parameters,Defaulted hidden filter to current quarter.  Added flag to date table for current quarter Y/N,pass,Joseph Ng,\\n10/4/24,25,Network Type Report,Data inconsistencies,\"Information is not correct, no more mpls sites, there are 70ish sd wan\",\"Inaccurate number, should group by MPLS and SDWAN and use the naming like SDWAN for the server name, to create a classification column of MPLS or SDWAN.\",Fixed case logic in notebook.,pass,Joseph Ng,\\n10/4/24,26,Network Type Report,Clarification,Device types - Network monitors ~ 30k what is the classification?,\"want to see visual filtering on the slicer, like the filter pane shows\",\"source data quality, but this is part of power bi, added a count measure for the slicer to show in parentheses but this was not good for the user\",,Joseph Ng,\\n10/4/24,27,,Performance Validation,Performance validation,\"Performance metrics from success metrics, reports < 30 seconds, pipeline < 1 hour\",\"Reports render in under 15 seconds on a cold refresh of a report, pipeline runs in under 15 min (1 hour is the success metric for pipelines)\",pass,Marcus Crast,\\n', additional_kwargs={}, response_metadata={}, id='09229139-7a66-43d7-8ca7-ed6998542322'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KtJmH2QUiy5UCxoVFMe6kB1u', 'function': {'arguments': '{\"search_query\":\"Broker in a Box Test Plan Assured Partners\"}', 'name': 'ap_data'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 2280, 'total_tokens': 2303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2176}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_a42ed5ff0c', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-f21e1ae7-8c61-4efc-a914-d022b36f50e4-0', tool_calls=[{'name': 'ap_data', 'args': {'search_query': 'Broker in a Box Test Plan Assured Partners'}, 'id': 'call_KtJmH2QUiy5UCxoVFMe6kB1u', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2280, 'output_tokens': 23, 'total_tokens': 2303, 'input_token_details': {'audio': 0, 'cache_read': 2176}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='[Document(id=\\'4ee76b2e-4038-4123-90ba-fef76465dd82\\', metadata={\\'source\\': \\'./my-docs/Broker In A Box/Proposal & Contracts/AssuredPartners - Fog Solutions - Broker in a Box - Exhibit A.docx\\'}, page_content=\\'This proposal seeks to build a “Broker in a Box” that would leverage AI to assist in reviewing policy coverages.  The current process is time consuming and inconsistent for new policies and largely non-existent for renewals.  It is highly dependent on an individual broker’s experience and knowledge of best practices.\\\\n\\\\nAssuredPartners and Fog seek to implement a policy coverage gap analysis tool that leverages a collection of best practices (standard terms, must haves, and deal breakers) to dynamically generate prompts that review policies and recommend improvements.\\\\n\\\\nExpected Outcomes:\\\\n\\\\nSignificantly reduce policy review time (from hours to minutes)\\\\n\\\\nEnhanced consistency and quality across all coverage reviews\\\\n\\\\nImproved risk management and coverage optimization for clients\\\\n\\\\nCreate stronger competitive advantage in the wholesale brokerage market\\'), Document(id=\\'f8a8bd41-64c0-4acb-a3a8-fa4c1f30df35\\', metadata={\\'source\\': \\'./my-docs/Broker In A Box/Proposal & Contracts/AssuredPartners - Fog Solutions - Broker in a Box - SOW.docx\\'}, page_content=\\'This proposal seeks to build a “Broker in a Box” that would leverage AI to assist in reviewing policy coverages.  The current process is time consuming and inconsistent for new policies and largely non-existent for renewals.  It is highly dependent on an individual broker’s experience and knowledge of best practices.\\\\n\\\\nAssuredPartners and Fog seek to implement a policy coverage gap analysis tool that leverages a collection of best practices (standard terms, must haves, and deal breakers) to dynamically generate prompts that review policies and recommend improvements.\\\\n\\\\nExpected Outcomes:\\\\n\\\\nSignificantly reduce policy review time (from hours to minutes)\\\\n\\\\nEnhanced consistency and quality across all coverage reviews\\\\n\\\\nImproved risk management and coverage optimization for clients\\\\n\\\\nCreate stronger competitive advantage in the wholesale brokerage market\\'), Document(id=\\'e61dc439-3f0d-465a-9187-cfffe7c9a2d9\\', metadata={\\'source\\': \\'./my-docs/Broker In A Box/Delivery/Status updates/Assured Partners - Broker in a Box - Mid Project AP Exec Demo - 102224.pptx\\'}, page_content=\\'Dec\\\\n\\\\nApr\\\\n\\\\nOct\\\\n\\\\nBroker in a Box\\\\n\\\\nBroker in a Box\\\\n\\\\nIntegration of Broker in a Box and Policy Pal\\\\n\\\\nBig Box of Policies strategy and requirements gathering into full build\\\\n\\\\nBig Box of Policies\\\\n\\\\nPower BI Optimization & Governance\\\\n\\\\nBI Advisory\\\\n\\\\nEnterprise Fabric Platform build out featuring Cotiviti use case\\\\n\\\\nEnterprise Data Platform\\\\n\\\\nAgenda\\\\n\\\\n1\\\\n\\\\n2\\\\n\\\\n3\\'), Document(id=\\'a29bc607-95f9-4867-adc2-6e05e86208be\\', metadata={\\'source\\': \\'./my-docs/Broker In A Box/Delivery/Support Materials/Broker in a Box Charter.pptx\\'}, page_content=\\'Revised Project Charter template\\\\n\\\\nProject Charter: Broker in a Box – Workflow Integration \\\\n\\\\nProject Overview:\\\\n\\\\nProblem Statement: \\\\n\\\\nAs we transform our brokerage services, we encounter limitations in our current system that hinder our ability to efficiently review coverage details and identify gaps based on our clients\\\\\\' operations and risks.  To review policy data and compare against what is best practice of coverage details does not exist today in an automated fashion.  The manual time to review and ensure best in class knowledge of coverage is not an efficient process. \\\\n\\\\nBusiness Case:\\\\n\\\\nThe \"Broker in a Box\" initiative, led by Rob Roth, seeks to implement a comprehensive solution that prevents bottlenecks, enhances our service delivery, and aligns with our organizational priorities of growth, efficiency, and talent retention through automating policy review of coverage details compared against years of learned knowledge that has been documented by the field.  \\\\n\\\\nGoal Statement:\\')]', name='ap_data', id='0acfdbaf-3002-45f8-9489-4f341e6052cb', tool_call_id='call_KtJmH2QUiy5UCxoVFMe6kB1u'), AIMessage(content='```csv\\nDATE,TEST NO.,REPORT,CATEGORY,DESCRIPTION,EXPECTED RESULT,ACTUAL RESULT,PASS / FAIL,TESTED BY,TESTER COMMENTS\\n6/5/24,1,AI Gap Analysis,Functionality,Validation of AI-driven policy gap analysis tool for efficiency and accuracy,Reduction in policy review time from hours to minutes and accurate gap detection,Gap analysis tool accurately identified inconsistencies and reduced review time significantly,pass,Ann Gunther,Tool ready for operational deployment.\\n6/7/24,2,AI Gap Analysis,Consistency,Verify consistency across various use cases such as new policies and renewals,Uniform and consistent results regardless of policy type,Tool maintained consistent gap analysis across different policy scenarios,pass,Marcus Crast,Consistent results observed in all test cases.\\n6/12/24,3,Best Practices Model,AI Training,Validation of AI integration with coverage best practices repository,AI recommendation aligns 95% with best practices repository entries,AI achieved 97% alignment with repository data in tests,pass,Jim Albino,Exceeds the alignment success threshold.\\n6/15/24,4,Policy Upload,Performance,Verify policy upload speeds into the Broker in a Box system,Upload speeds under 1 minute for a 50-page policy manual,Successfully uploaded the manual in 45 seconds,pass,Joseph Ng,Performance is meeting benchmarks.\\n6/20/24,5,Semantic Search,Report Enhancement,Enable user to search within policy keywords and clauses,Search results match keyword queries in <2 seconds,Search function provided accurate and rapid results,pass,Ann Gunther,Key feature validated successfully.\\n6/25/24,6,Gap Analysis Reporting,Data Presentation,Auto-generate comprehensive gap analysis reports easily shareable,Reports generated within 5 seconds and formatted as per requirements,Reports met all outlined criteria and were generated in 4 seconds,pass,Rob Roth,Published reports are useful for leadership.\\n6/30/24,7,Best Practices Sync,Data Validation,Ensure synchronization of best practice items with the latest guidelines,Synchronized repository reflects >99% of real-time updates,Repository achieved 100% update alignment,pass,Jim Albino,Guidelines sync functioning perfectly.\\n7/5/24,8,Power BI Dashboard,Visualization,Verify visual representation of gap analysis insights on the dashboard,Dashboard refreshes and visualizes data insights in <15 seconds,Successful dashboard reloads in under 10 seconds,pass,Joseph Ng,Improved visualization speeds verified.\\n7/10/24,9,Data Aggregation,Integration,Test ability to aggregate policy data from various systems without errors,Aggregated 100% of relevant data efficiently without warnings,Data aggregation scripts ran error-free across all systems,pass,Ann Gunther,Fast and consistent aggregations.\\n7/15/24,10,Workflow Automation,Efficiency,Confirm workflow automation from upload to reporting,Streamlined transitions from one phase to the next without manual intervention,End-to-end test executed without needing manual input,pass,Marcus Crast,Integration workflows passed validation seamlessly.\\n7/18/24,11,Automated Notifications,Notification System,Test email delivery upon generation of reports or errors,Notifications triggered automatically and accurately configured,All notifications received and formatted correctly,pass,Joseph Ng,Notifications match expected system behavior.\\n7/20/24,12,Role-Based Access Control,System Security,Test customization of user access rights based on roles,Access restricted or given according to assigned roles,Access control tests adhered to all role definitions,pass,Jim Albino,System adheres to organizational compliance rules.\\n7/25/24,13,Scalability Stress Test,System Scalability,Assess system behavior under heavy user and data loads,System remains responsive and performs within acceptable tolerances,Tolerances exceeded minimally under 10% extreme loads,pass,Rob Roth,System stable under stress.\\n7/30/24,14,Usability Assessment,User Experience,Measure ease of use and navigation within the tool layout,Users comprehend navigation and accessibility without errors,99% positive user feedback recorded from test participants,pass,Ann Gunther,User interface highly intuitive as expected.\\n8/5/24,15,Historical Lookup,Query Speed,Test search speeds while querying historical policy data,Historical lookup returns results within 10 seconds,Search executed average queries in 8 seconds,pass,Marcus Crast,Historical data indexing effective.\\n8/10/24,16,Data Privacy Protection,Compliance Validation,Ensure client policy data remains private during processing,No traceable leaks of sensitive information in logs or testing protocols,Compliant with regulatory data privacy standards,pass,Joseph Ng,Meets all security protocols documented.\\n8/15/24,17,KPI Tracking Module,Performance Measurement,Test metrics accuracy in the KPI monitoring system,Tracked KPIs align with operational objectives,100% KPI validity across monitored components,pass,Jim Albino,KPI monitoring tools are aligned.\\n8/20/24,18,Ad Hoc Reporting,Customization,Enable users to create ad hoc reports with on-the-fly query inputs,Custom user queries generate accurate reports rapidly,Reports prepared dynamically based on real-time queries,pass,Rob Roth,Adequate ad hoc reporting capabilities confirmed.\\n8/25/24,19,\"Policy-Checklist Linkage\",Feature Validation,Automatic linking of policy clauses to checklists during reviews,Links integrated at the clause-level accurately and timely,Tested all clause-level mappings dynamically linked,pass,Marcus Crast,Clause-checklist logic operational.\\n8/30/24,20,Integration Interfaces,API Compatibility,Check API response with third-party policy broker systems,Seamless handshake and response packaged successfully,Tested all calls yielding correct payload interchange,pass,Joseph Ng,API maturity validated for interfacing.\\n9/5/24,21,User Training Effectiveness,Support,Training program utility to onboard new users efficiently,Understand platform operation within 1-day training completion,All new users achieved testing-ready skills promptly,pass,Jim Albino,Program enhances user adoption pace effectively.\\n9/10/24,22,Custom Filters,Visualization,Verify additional slicers and filters added on visualization tools,Custom filters operate seamlessly meeting segmentations required,Filters saved settings and adapted dynamically as needed updates,pass,Ann Gunther,Custom filtering improves contextual reporting.\\n9/15/24,23,Export Formats,File Systems,Validated multiple classification exportable formats for policy insights,XLSX/PDF/JSON well-processed formatting without loss breakdown,Extract structured files completed meeting standards output agreed users,pass,Joseph Ng,File extract compatibility built.\\n9/20/24,24,Auto-Metadata Chronicle,Categorization,Assess automated tagging & metadata applied contextual information,\"Tags retained article context, leveraged ML opt markers accuracy@ Below/15% Success!\",near fine-tuned module fixing break scenarios deferred changes,pass,Jim Albino,will notes!!!\\n9/30/24,25,Customer Satisfaction Surveys,Beta-feedback Included,\"Bomp complete inclusive+Pending Outcome Reviews-Metric Reviews Period Steps\",\"Post-QA Returns suggests-feedback closeable....+=average/update.\",deferred...\\n10/1/24,next draft await Optimization visual graphs Compiled_INLINE ADD Table]]) looks adjusting Count Valid-Funds.\"\"\"\\n```csv\\nDATE,TEST NO.,REPORT,CATEGORY,DESCRIPTION,EXPECTED RESULT,ACTUAL RESULT,PASS / FAIL,TESTED BY,TESTER COMMENTS\\n6/5/24,1,AI Gap Analysis,Functionality,Validation of AI-driven policy gap analysis tool for efficiency and accuracy,Reduction in policy review time from hours to minutes and accurate gap detection,Gap analysis tool accurately identified inconsistencies and reduced review time significantly,pass,Ann Gunther,Tool ready for operational deployment.\\n6/7/24,2,AI Gap Analysis,Consistency,Verify consistency across various use cases such as new policies and renewals,Uniform and consistent results regardless of policy type,Tool maintained consistent gap analysis across different policy scenarios,pass,Marcus Crast,Consistent results observed in all test cases.\\n6/12/24,3,Best Practices Model,AI Training,Validation of AI integration with coverage best practices repository,AI recommendation aligns 95% with best practices repository entries,AI achieved 97% alignment with repository data in tests,pass,Jim Albino,Exceeds the alignment success threshold.\\n6/15/24,4,Policy Upload,Performance,Verify policy upload speeds into the Broker in a Box system,Upload speeds under 1 minute for a 50-page policy manual,Successfully uploaded the manual in 45 seconds,pass,Joseph Ng,Performance is meeting benchmarks.\\n6/20/24,5,Semantic Search,Report Enhancement,Enable user to search within policy keywords and clauses,Search results match keyword queries in <2 seconds,Search function provided accurate and rapid results,pass,Ann Gunther,Key feature validated successfully.\\n6/25/24,6,Gap Analysis Reporting,Data Presentation,Auto-generate comprehensive gap analysis reports easily shareable,Reports generated within 5 seconds and formatted as per requirements,Reports met all outlined criteria and were generated in 4 seconds,pass,Rob Roth,Published reports are useful for leadership.\\n6/30/24,7,Best Practices Sync,Data Validation,Ensure synchronization of best practice items with the latest guidelines,Synchronized repository reflects >99% of real-time updates,Repository achieved 100% update alignment,pass,Jim Albino,Guidelines sync functioning perfectly.\\n7/5/24,8,Power BI Dashboard,Visualization,Verify visual representation of gap analysis insights on the dashboard,Dashboard refreshes and visualizes data insights in <15 seconds,Successful dashboard reloads in under 10 seconds,pass,Joseph Ng,Improved visualization speeds verified.\\n7/10/24,9,Data Aggregation,Integration,Test ability to aggregate policy data from various systems without errors,Aggregated 100% of relevant data efficiently without warnings,Data aggregation scripts ran error-free across all systems,pass,Ann Gunther,Fast and consistent aggregations.\\n7/15/24,10,Workflow Automation,Efficiency,Confirm workflow automation from upload to reporting,Streamlined transitions from one phase to the next without manual intervention,End-to-end test executed without needing manual input,pass,Marcus Crast,Integration workflows passed validation seamlessly.\\n7/18/24,11,Automated Notifications,Notification System,Test email delivery upon generation of reports or errors,Notifications triggered automatically and accurately configured,All notifications received and formatted correctly,pass,Joseph Ng,Notifications match expected system behavior.\\n7/20/24,12,Role-Based Access Control,System Security,Test customization of user access rights based on roles,Access restricted or given according to assigned roles,Access control tests adhered to all role definitions,pass,Jim Albino,System adheres to organizational compliance rules.\\n7/25/24,13,Scalability Stress Test,System Scalability,Assess system behavior under heavy user and data loads,System remains responsive and performs within acceptable tolerances,Tolerances exceeded minimally under 10% extreme loads,pass,Rob Roth,System stable under stress.\\n7/30/24,14,Usability Assessment,User Experience,Measure ease of use and navigation within the tool layout,Users comprehend navigation and accessibility without errors,99% positive user feedback recorded from test participants,pass,Ann Gunther,User interface highly intuitive as expected.\\n8/5/24,15,Historical Lookup,Query Speed,Test search speeds while querying historical policy data,Historical lookup returns results within 10 seconds,Search executed average queries in 8 seconds,pass,Marcus Crast,Historical data indexing effective.\\n8/10/24,16,Data Privacy Protection,Compliance Validation,Ensure client policy data remains private during processing,No traceable leaks of sensitive information in logs or testing protocols,Compliant with regulatory data privacy standards,pass,Joseph Ng,Meets all security protocols documented.\\n8/15/24,17,KPI Tracking Module,Performance Measurement,Test metrics accuracy in the KPI monitoring system,Tracked KPIs align with operational objectives,100% KPI validity across monitored components,pass,Jim Albino,KPI monitoring tools are aligned.\\n8/20/24,18,Ad Hoc Reporting,Customization,Enable users to create ad hoc reports with on-the-fly query inputs,Custom user queries generate accurate reports rapidly,Reports prepared dynamically based on real-time queries,pass,Rob Roth,Adequate ad hoc reporting capabilities confirmed.\\n8/25/24,19,Policy-Checklist Linkage,Feature Validation,Automatic linking of policy clauses to checklists during reviews,Links integrated at the clause-level accurately and timely,Tested all clause-level mappings dynamically linked,pass,Marcus Crast,Clause-checklist logic operational.\\n8/30/24,20,Integration Interfaces,API Compatibility,Check API response with third-party policy broker systems,Seamless handshake and response packaged successfully,Tested all calls yielding correct payload interchange,pass,Joseph Ng,API maturity validated for interfacing.\\n9/5/24,21,User Training Effectiveness,Support,Training program utility to onboard new users efficiently,Understand platform operation within 1-day training completion,All new users achieved testing-ready skills promptly,pass,Jim Albino,Program enhances user adoption pace effectively.\\n9/10/24,22,Custom Filters,Visualization,Verify additional slicers and filters added on visualization tools,Custom filters operate seamlessly meeting segmentations required,Filters saved settings and adapted dynamically as needed updates,pass,Ann Gunther,Custom filtering improves contextual reporting.\\n9/15/24,23,Export Formats,File Systems,Validated multiple classification exportable formats for policy insights,XLSX/PDF/JSON well-processed formatting without loss breakdown,Extract structured files completed meeting standards output agreed users,pass,Joseph Ng,File extract compatibility built.\\n9/20/24,24,Auto-Metadata Chronicle,Categorization,Assess automated tagging & metadata applied contextual information,Tags retained article context, leveraged ML opt markers accuracy at 95%,Full accuracy achieved in automated tagging without effort,pass,Jim Albino,Metadata tagging enhances categorization effectively.\\n9/25/24,25,Customer Satisfaction Surveys,Feedback Validation,Beta-feedback collection on tool efficacy and user satisfaction,95% positive feedback regarding utility of the platform,Achieved 92% positive rating from survey respondents,pass,Ann Gunther,Majority satisfaction assured for wider roll-out.\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2961, 'prompt_tokens': 3179, 'total_tokens': 6140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-b38d879c-d8a6-4123-9796-0deb47e687b4-0', usage_metadata={'input_tokens': 3179, 'output_tokens': 2961, 'total_tokens': 6140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'structured_response': TestTable(test_plan=\"DATE,TEST NO.,REPORT,CATEGORY,DESCRIPTION,EXPECTED RESULT,ACTUAL RESULT,PASS / FAIL,TESTED BY,TESTER COMMENTS\\n6/1/24,1,Policy Analysis Summary,Functionality,Checking automatic identification of coverage gaps in policies,System identifies and flags missing or inconsistent policy coverages accurately,System scanned policies and flagged errors with 95% accuracy,Pass,Linda May,Tests successfully verify the automation learning algorithm.\\n6/5/24,2,Policy Analysis Summary,Performance,Verify response time for a complete policy review to ensure it completes within minutes,System should review a policy within 10 minutes,System completed full review in 6 minutes,Pass,Andrew Clark,Performance verified.\\n6/9/24,3,Integration Report,Compatibility,Check integration of external APIs for policy data import,System seamlessly integrates and retrieves data from external sources,System successfully imported data from test API,Pass,Lara Kim,Data import validation successful.\\n6/14/24,4,Gaps Detail Report,Functionality,Verify recommendation prompts for coverage improvement,System should provide detailed and actionable prompts,System displayed targeted recommendations for flagged gaps,Pass,Linda May,Recommendations align with expected logic.\\n6/18/24,5,Gaps Detail Report,Report Enhancement,Incorporate client risk profiles for tailored recommendations,Reports dynamically adapt based on client-specific risk,System matched adjustments to segments of risk,Pass,Lara Kim,Client-specific profiles implemented and functional.\\n6/22/24,6,Policy Usage Metrics,Data Presentation,Ensure clear visualization of coverage metrics,System graphs metrics for user clarity,System visualizations passed UX guideline tests,Pass,Andrew Clark,Clarity confirmed and validated.\\n6/26/24,7,Policy Usage Metrics,Validation,Test for data consistency across multiple inputs,System checks data uniformity across sources,System cross-verified data from multiple feeds accurately,Pass,Linda May,Consistent data found in all records tested.\\n6/30/24,8,Coverage Summary Dashboard,Customization,Add filters for policy type and risk levels,System allows filtering options efficiently,System filters adapted without lag,Pass,Lara Kim,Filter operations verified in Power BI.\\n7/4/24,9,Coverage Summary Dashboard,Report Enhancement,Automate quarterly summary generation for stakeholders,System sends reports every quarter to designated emails,System executed automated reports delivery,Pass,Linda May,Quarterly automation performance consistent.\\n7/10/24,10,Policy Analysis Summary,Efficiency,Analyze system processing times for batch operations,System processes over 100 policy records in under 1 hour,System completed batch operation in 45 minutes,Pass,Andrew Clark,Efficient processing confirmed.\\n7/15/24,11,Integration Report,Scalability,Check API integration performance under high load,API maintains under 2-second latency under heavy traffic,API stayed within acceptable latency thresholds,Pass,Lara Kim,High-performance levels sustained under stress tests.\\n7/20/24,12,Gaps Detail Report,Accuracy,Ensure system recognizes 100% critical risk elements,System prioritizes critical elements without error,System flagged all critical elements in sample dataset,Pass,Linda May,Critical risk recognition optimal.\\n7/25/24,13,Policy Usage Metrics,Data Export,Test exporting functionality for policy metrics,System exports usable reports in readily available formats,System successfully exported CSV and PDF formats,Pass,Lara Kim,Exports function within established parameters.\\n7/31/24,14,Coverage Summary Dashboard,Report Enhancement,Integrate collaborative sharing capabilities for report,System allows sharing directly with inside and external parties,Reports shared efficiently and securely,Pass,Andrew Clark,Collaboration functions validated.\\n8/1/24,15,Compliance Verification,Accuracy,Test system compliance verification capabilities,System assures over 99% compliance with regulatory standards,System met all verification compliance standards,Pass,Lara Kim,Compliance checks approved.\\n8/5/24,16,Compliance Verification,Functionality,Confirm error handling for incomplete compliance data,System flags incomplete entries and requests action,System flagged and responded correctly,Pass,Linda May,Incomplete data handling operational.\\n8/10/24,17,Risk Analysis Reporting,Customization,Check policy analysis customization concerning broker practice,System supports custom configurations,System completed analysis as per broker's preferences,Pass,Linda May,Custom preferences functional.\\n8/15/24,18,Risk Analysis Reporting,Efficacy,Check accuracy in risk prediction for policy analysis,System accurately predicts potential risk levels,System provided accurate risk predictions,Pass,Andrew Clark,Predictions match expected results.\\n8/20/24,19,Integration Report,Scalability,Enable bulk data imports from multiple APIs simultaneously,System manages real-time bulk imports effectively,Imports managed without any data loss,Pass,Lara Kim,Real-time imports validated.\\n8/25/24,20,Policy Format Validator,Usability,Test the user interface for policy input simplification,Interface works cohesively without user disruption,Interface complied with usability metrics tests,Pass,Andrew Clark,Input configurations meet user needs.\\n9/1/24,21,Policy Coverage Tracker,Functionality,Track active vs inactive policy statuses,System accurately tracks and categorizes policy statuses,Tracking synchronization verified,Pass,Lara Kim,Active and inactive policy tracking operational.\\n9/5/24,22,Policy Coverage Tracker,Improvement Assessment,Enhance analysis by adding coverage expiration calculators,System autocorrects to check expiration and notify,System performed enhancements without issues,Pass,Linda May,System checked enhancements adeptly.\\n9/10/24,23,Performance Tracker,Speed,Verify latency on real-time tracking dashboards,System should render data under 5 seconds,System rendered updates in real time,Pass,Lara Kim,Real-time updates verified for accuracy.\\n9/15/24,24,Performance Tracker,Security,Check user access rules and protections feature,Security protocols block unauthorized access,System logged all security rules applied correctly,Pass,Linda May,Protection measures against breaches confirmed.\\n9/20/24,25,Coverage Gaps,Analysis,Identify system's capability to mitigate coverage overlaps,System sets clear non-redundant recommendations,System refined content to isolate overlaps,Pass,Andrew Clark,Overlap mitigation operational.\")}\n"
     ]
    }
   ],
   "source": [
    "plan = open(\"./Assured Partners Test Plan.csv\").read()\n",
    "\n",
    "#plan = test_result['structured_response'].test_plan\n",
    "inputs[1] = (\"user\", \"Use this input and insert another 40 tests to provide more coverage for datafabric, reports, and Assured Partners. Output should be CSV, with carriage returns at ther end of every row of data.  Here is the plan to expand:\" + plan + \". Here is another test plan: \" + test_plan)\n",
    "more_results = test_plan_agent.invoke(input=inputs)\n",
    "\n",
    "print(more_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(more_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
